diff --git a/oneDNN b/oneDNN
index aafb722..9aa7e3e 160000
--- a/oneDNN
+++ b/oneDNN
@@ -1 +1 @@
-Subproject commit aafb722ea6809a7ac622758becf895cc8ac93432
+Subproject commit 9aa7e3e9366b5d0853f0266bd01bcff20a2e9d28
diff --git a/src/fp32_matmul_softmax_fusion_100111.cpp b/src/fp32_matmul_softmax_fusion_100111.cpp
new file mode 100644
index 0000000..b6681a6
--- /dev/null
+++ b/src/fp32_matmul_softmax_fusion_100111.cpp
@@ -0,0 +1,257 @@
+#include <runtime/kernel_include/cpu_include.hpp>
+
+#include <omp.h>
+#define sc_get_thread_id omp_get_thread_num
+#define sc_parallel_call_cpu_with_env sc_parallel_call_cpu_with_env_impl
+static bool outerloop_8_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_6(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ mul_12_outs_0, float* __restrict__ reorder_1_ins_0, float* __restrict__ matmul_core_3_ins_0, float* __restrict__ mul_5_ins_1, float* __restrict__ add_7_ins_0) noexcept __attribute__((nonnull (2,3,4,5,6,7)));
+extern "C" void sc_parallel_call_cpu_with_env(void* func, uint64_t flags, void* stream, int8_t* env, uint64_t begin, uint64_t end, uint64_t step, generic_val* args) noexcept;
+static void outerloop_8_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_60_closure_0_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
+/**
+ * fp32_matmul_softmax_fusion_100111
+ * @param __stream the stream pointer, usually get_default_stream()
+ * @param __module_data the module global data
+ * @param logical_tensor_125 [f32 [8, 48, 48] @ ABC]
+ * @param logical_tensor_112 [f32 [8, 48, 32] @ ABC]
+ * @param logical_tensor_121 [f32 [8, 48, 32] @ ABC]
+ * @param logical_tensor_77 [f32 [1] @ A]
+ * @param logical_tensor_97 [f32 [8, 48, 48] @ ABC]
+*/
+static void fp32_matmul_softmax_fusion_100111(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ logical_tensor_125, float* __restrict__ logical_tensor_112, float* __restrict__ logical_tensor_121, float* __restrict__ logical_tensor_77, float* __restrict__ logical_tensor_97) noexcept __attribute__((nonnull (2,3,4,5,6,7)));
+extern "C" void* dnnl_brgemm_func(int32_t M, int32_t N, int32_t K, int32_t LDA, int32_t LDB, int32_t LDC, int32_t stride_a, int32_t stride_b, float beta, int32_t dtypeA, int32_t dtypeB, void* brg_attrs, void* bd_mask, void* postops_setting) noexcept;
+extern "C" void* sc_thread_aligned_malloc(void* stream, uint64_t size) noexcept __attribute__((returns_nonnull))  __attribute__((malloc));
+extern "C" void dnnl_brgemm_call(void* func, void* A, void* B, void* C, int32_t num, void* stream) noexcept;
+extern "C" void sc_thread_aligned_free(void* stream, void* ptr) noexcept;
+static void outerloop_8_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_60_closure_0(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0, float* __restrict__ reorder_1_ins_0, float* __restrict__ matmul_core_3_ins_0, float* __restrict__ mul_5_ins_1, float* __restrict__ add_7_ins_0, float* __restrict__ mul_12_outs_0) noexcept __attribute__((nonnull (2,4,5,6,7,8)));
+
+
+static void fp32_matmul_softmax_fusion_100111(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ logical_tensor_125, float* __restrict__ logical_tensor_112, float* __restrict__ logical_tensor_121, float* __restrict__ logical_tensor_77, float* __restrict__ logical_tensor_97) noexcept{
+  outerloop_8_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_6(__stream, __module_data, logical_tensor_125, &logical_tensor_121[0UL], logical_tensor_112, logical_tensor_77, logical_tensor_97);
+}
+
+static bool outerloop_8_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_6(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ mul_12_outs_0, float* __restrict__ reorder_1_ins_0, float* __restrict__ matmul_core_3_ins_0, float* __restrict__ mul_5_ins_1, float* __restrict__ add_7_ins_0) noexcept{
+  generic_val __tempargs0[5UL];
+  __tempargs0[0UL] = reorder_1_ins_0;
+  __tempargs0[1UL] = matmul_core_3_ins_0;
+  __tempargs0[2UL] = mul_5_ins_1;
+  __tempargs0[3UL] = add_7_ins_0;
+  __tempargs0[4UL] = mul_12_outs_0;
+  sc_parallel_call_cpu_with_env((void*)&outerloop_8_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_60_closure_0_0wrapper, 0UL, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs0);
+  return true;
+}
+
+extern "C" void fp32_matmul_softmax_fusion_100111_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
+  generic_val __cached_0;
+  __cached_0 = args[0UL];
+  generic_val __cached_1;
+  __cached_1 = args[1UL];
+  generic_val __cached_2;
+  __cached_2 = args[2UL];
+  generic_val __cached_3;
+  __cached_3 = args[3UL];
+  generic_val __cached_4;
+  __cached_4 = args[4UL];
+  fp32_matmul_softmax_fusion_100111(__stream, __module_data, (float*)(__cached_0.v_ptr), (float*)(__cached_1.v_ptr), (float*)(__cached_2.v_ptr), (float*)(__cached_3.v_ptr), (float*)(__cached_4.v_ptr));
+}
+
+static void outerloop_8_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_6_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
+  generic_val __cached_0;
+  __cached_0 = args[0UL];
+  generic_val __cached_1;
+  __cached_1 = args[1UL];
+  generic_val __cached_2;
+  __cached_2 = args[2UL];
+  generic_val __cached_3;
+  __cached_3 = args[3UL];
+  generic_val __cached_4;
+  __cached_4 = args[4UL];
+  outerloop_8_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_6(__stream, __module_data, (float*)(__cached_0.v_ptr), (float*)(__cached_1.v_ptr), (float*)(__cached_2.v_ptr), (float*)(__cached_3.v_ptr), (float*)(__cached_4.v_ptr));
+}
+
+static vec_f32x16 _should_inline_exp_f32x16(void* __stream, int8_t* __restrict__ __module_data, vec_f32x16 _intrin_v0) noexcept{
+  vec_f32x16 a_;
+  a_ = sc_min(_intrin_v0, vec_f32x16(88.5999985));
+  vec_f32x16 k_float;
+  k_float = sc_floor((a_ * vec_f32x16(1.44269502)));
+  vec_s32x16 k_int;
+  k_int = (vec_s32x16)(k_float);
+  vec_f32x16 r;
+  r = (a_ - (k_float * vec_f32x16(0.693147182)));
+  vec_f32x16 Tn;
+  Tn = vec_f32x16(1.f);
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.166666672)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.200000003)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.25)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.333333343)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.5)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(1.f)), vec_f32x16(1.f));
+  vec_s32x16 result;
+  result = ((k_int << vec_s32x16(23)) + sc_reinterpret<vec_s32x16>(Tn));
+  return sc_select((_intrin_v0 >= vec_f32x16(-87.3300018)), sc_reinterpret<vec_f32x16>(result), vec_f32x16(0.f));
+}
+
+extern "C" void sc_init_fp32_matmul_softmax_fusion_100111(void* __stream, int8_t* __restrict__ __module_data) noexcept{
+  void*& __sc_kernel_cache = *(void**)(__module_data + 0);
+  __sc_kernel_cache = dnnl_brgemm_func(48, 48, 32, 32, 48, 48, 32, 1536, 0.f, 4, 4, ((void*)0), ((void*)0), ((void*)0));
+}
+
+static void outerloop_8_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_60_closure_0(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0, float* __restrict__ reorder_1_ins_0, float* __restrict__ matmul_core_3_ins_0, float* __restrict__ mul_5_ins_1, float* __restrict__ add_7_ins_0, float* __restrict__ mul_12_outs_0) noexcept{
+  void*& __sc_kernel_cache = *(void**)(__module_data + 0);
+  int8_t* __rescheduled_1 = (int8_t*)sc_thread_aligned_malloc(__stream, 15360UL);
+  float* reorder_1_outs_0_shr = (float*)&__rescheduled_1[0UL];
+  float* matmul_core_3_outs_0_shr = (float*)&__rescheduled_1[6144UL];
+  for (uint64_t _fuseiter_31 = 0UL; _fuseiter_31 < 32UL; _fuseiter_31 += 8UL) {
+    for (uint64_t _fuseiter_32 = 0UL; _fuseiter_32 < 48UL; _fuseiter_32 += 8UL) {
+      vec_f32x8 row1_13;
+      vec_f32x8 row2_14;
+      vec_f32x8 row3_15;
+      vec_f32x8 row4_16;
+      vec_f32x8 row5_17;
+      vec_f32x8 row6_18;
+      vec_f32x8 row7_19;
+      vec_f32x8 row8_20;
+      vec_f32x8 row9_21;
+      vec_f32x8 row10_22;
+      vec_f32x8 row11_23;
+      vec_f32x8 row12_24;
+      vec_f32x8 __cached_0;
+      __cached_0 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + ((_fuseiter_32 * 32UL) + _fuseiter_31))]);
+      row1_13 = __cached_0;
+      vec_f32x8 __cached_1;
+      __cached_1 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_32 + 1UL) * 32UL) + _fuseiter_31))]);
+      row2_14 = __cached_1;
+      vec_f32x8 __cached_2;
+      __cached_2 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_32 + 2UL) * 32UL) + _fuseiter_31))]);
+      row3_15 = __cached_2;
+      vec_f32x8 __cached_3;
+      __cached_3 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_32 + 3UL) * 32UL) + _fuseiter_31))]);
+      row4_16 = __cached_3;
+      vec_f32x8 __cached_4;
+      __cached_4 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_32 + 4UL) * 32UL) + _fuseiter_31))]);
+      row5_17 = __cached_4;
+      vec_f32x8 __cached_5;
+      __cached_5 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_32 + 5UL) * 32UL) + _fuseiter_31))]);
+      row6_18 = __cached_5;
+      vec_f32x8 __cached_6;
+      __cached_6 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_32 + 6UL) * 32UL) + _fuseiter_31))]);
+      row7_19 = __cached_6;
+      vec_f32x8 __cached_7;
+      __cached_7 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_32 + 7UL) * 32UL) + _fuseiter_31))]);
+      row8_20 = __cached_7;
+      row9_21 = sc_unpack_low(row1_13, row2_14, 32);
+      row1_13 = sc_unpack_high(row1_13, row2_14, 32);
+      row10_22 = sc_unpack_low(row3_15, row4_16, 32);
+      row2_14 = sc_unpack_high(row3_15, row4_16, 32);
+      row11_23 = sc_unpack_low(row5_17, row6_18, 32);
+      row3_15 = sc_unpack_high(row5_17, row6_18, 32);
+      row12_24 = sc_unpack_low(row7_19, row8_20, 32);
+      row4_16 = sc_unpack_high(row7_19, row8_20, 32);
+      row5_17 = sc_shuffle(row9_21, row10_22, 68);
+      row6_18 = sc_shuffle(row9_21, row10_22, 238);
+      row7_19 = sc_shuffle(row1_13, row2_14, 68);
+      row8_20 = sc_shuffle(row1_13, row2_14, 238);
+      row9_21 = sc_shuffle(row11_23, row12_24, 68);
+      row10_22 = sc_shuffle(row11_23, row12_24, 238);
+      row11_23 = sc_shuffle(row3_15, row4_16, 68);
+      row12_24 = sc_shuffle(row3_15, row4_16, 238);
+      row1_13 = sc_permute(row5_17, row9_21, 32);
+      row2_14 = sc_permute(row6_18, row10_22, 32);
+      row3_15 = sc_permute(row7_19, row11_23, 32);
+      row4_16 = sc_permute(row8_20, row12_24, 32);
+      row5_17 = sc_permute(row5_17, row9_21, 49);
+      row6_18 = sc_permute(row6_18, row10_22, 49);
+      row7_19 = sc_permute(row7_19, row11_23, 49);
+      row8_20 = sc_permute(row8_20, row12_24, 49);
+      vec_f32x8 __cached_8;
+      __cached_8 = row1_13;
+      vec_f32x8::store(__cached_8, &reorder_1_outs_0_shr[((_fuseiter_31 * 48UL) + _fuseiter_32)]);
+      vec_f32x8 __cached_9;
+      __cached_9 = row2_14;
+      vec_f32x8::store(__cached_9, &reorder_1_outs_0_shr[(((_fuseiter_31 + 1UL) * 48UL) + _fuseiter_32)]);
+      vec_f32x8 __cached_10;
+      __cached_10 = row3_15;
+      vec_f32x8::store(__cached_10, &reorder_1_outs_0_shr[(((_fuseiter_31 + 2UL) * 48UL) + _fuseiter_32)]);
+      vec_f32x8 __cached_11;
+      __cached_11 = row4_16;
+      vec_f32x8::store(__cached_11, &reorder_1_outs_0_shr[(((_fuseiter_31 + 3UL) * 48UL) + _fuseiter_32)]);
+      vec_f32x8 __cached_12;
+      __cached_12 = row5_17;
+      vec_f32x8::store(__cached_12, &reorder_1_outs_0_shr[(((_fuseiter_31 + 4UL) * 48UL) + _fuseiter_32)]);
+      vec_f32x8 __cached_13;
+      __cached_13 = row6_18;
+      vec_f32x8::store(__cached_13, &reorder_1_outs_0_shr[(((_fuseiter_31 + 5UL) * 48UL) + _fuseiter_32)]);
+      vec_f32x8 __cached_14;
+      __cached_14 = row7_19;
+      vec_f32x8::store(__cached_14, &reorder_1_outs_0_shr[(((_fuseiter_31 + 6UL) * 48UL) + _fuseiter_32)]);
+      vec_f32x8 __cached_15;
+      __cached_15 = row8_20;
+      vec_f32x8::store(__cached_15, &reorder_1_outs_0_shr[(((_fuseiter_31 + 7UL) * 48UL) + _fuseiter_32)]);
+    }
+  }
+  dnnl_brgemm_call(__sc_kernel_cache, &matmul_core_3_ins_0[(__itr_0 * 1536UL)], &reorder_1_outs_0_shr[0UL], &matmul_core_3_outs_0_shr[0UL], 1, __stream);
+  float* mul_5_outs_0_shr = (float*)&__rescheduled_1[0UL];
+  float* reciprocal_10_outs_0_shr = (float*)&__rescheduled_1[192UL];
+  for (uint64_t __inner_itr_1 = 0UL; __inner_itr_1 < 48UL; __inner_itr_1 += 1UL) {
+    vec_f32x16 reduce_compute_13_outs_0_shr0;
+    vec_f32x16 __cached_16;
+    __cached_16 = vec_f32x16(0.f);
+    reduce_compute_13_outs_0_shr0 = __cached_16;
+    float* reduce_collect_14_outs_0_shr = (float*)&__rescheduled_1[256UL];
+    float __cached_17;
+    __cached_17 = 0.f;
+    reduce_collect_14_outs_0_shr[0UL] = __cached_17;
+    for (uint64_t _fuseiter_35 = 0UL; _fuseiter_35 < 48UL; _fuseiter_35 += 16UL) {
+      vec_f32x16 __cached_18;
+      __cached_18 = vec_f32x16::load(&matmul_core_3_outs_0_shr[((__inner_itr_1 * 48UL) + _fuseiter_35)]);
+      vec_f32x16 __cached_19;
+      __cached_19 = (__cached_18 * vec_f32x16(mul_5_ins_1[0]));
+      vec_f32x16 __cached_20;
+      __cached_20 = vec_f32x16::load(&add_7_ins_0[(((__itr_0 * 2304UL) + (__inner_itr_1 * 48UL)) + _fuseiter_35)]);
+      __cached_19 = (__cached_20 + __cached_19);
+      vec_f32x16 _retval1;
+      vec_f32x16 a_;
+      a_ = sc_min(__cached_19, vec_f32x16(88.5999985));
+      vec_f32x16 k_float;
+      k_float = sc_floor((a_ * vec_f32x16(1.44269502)));
+      vec_s32x16 k_int;
+      k_int = (vec_s32x16)(k_float);
+      vec_f32x16 r;
+      r = (a_ - (k_float * vec_f32x16(0.693147182)));
+      vec_f32x16 Tn;
+      Tn = vec_f32x16(1.f);
+      Tn = sc_fmadd(Tn, (r * vec_f32x16(0.166666672)), vec_f32x16(1.f));
+      Tn = sc_fmadd(Tn, (r * vec_f32x16(0.200000003)), vec_f32x16(1.f));
+      Tn = sc_fmadd(Tn, (r * vec_f32x16(0.25)), vec_f32x16(1.f));
+      Tn = sc_fmadd(Tn, (r * vec_f32x16(0.333333343)), vec_f32x16(1.f));
+      Tn = sc_fmadd(Tn, (r * vec_f32x16(0.5)), vec_f32x16(1.f));
+      Tn = sc_fmadd(Tn, (r * vec_f32x16(1.f)), vec_f32x16(1.f));
+      vec_s32x16 result;
+      result = ((k_int << vec_s32x16(23)) + sc_reinterpret<vec_s32x16>(Tn));
+      _retval1 = sc_select((__cached_19 >= vec_f32x16(-87.3300018)), sc_reinterpret<vec_f32x16>(result), vec_f32x16(0.f));
+      __cached_19 = _retval1;
+      vec_f32x16::store(__cached_19, &mul_5_outs_0_shr[_fuseiter_35]);
+      vec_f32x16 __cached_21;
+      __cached_21 = reduce_compute_13_outs_0_shr0;
+      __cached_21 = (__cached_21 + __cached_19);
+      reduce_compute_13_outs_0_shr0 = __cached_21;
+    }
+    vec_f32x16 __cached_22;
+    __cached_22 = reduce_compute_13_outs_0_shr0;
+    float __cached_23;
+    __cached_23 = sc_reduce_add(__cached_22);
+    reduce_collect_14_outs_0_shr[0UL] = __cached_23;
+    reciprocal_10_outs_0_shr[0UL] = (1.f / __cached_23);
+    for (uint64_t _fuseiter_59 = 0UL; _fuseiter_59 < 48UL; _fuseiter_59 += 16UL) {
+      vec_f32x16 __cached_24;
+      __cached_24 = vec_f32x16::load(&mul_5_outs_0_shr[_fuseiter_59]);
+      vec_f32x16 __cached_25;
+      __cached_25 = (__cached_24 * vec_f32x16(reciprocal_10_outs_0_shr[0UL]));
+      vec_f32x16::store(__cached_25, &mul_12_outs_0[(((__itr_0 * 2304UL) + (__inner_itr_1 * 48UL)) + _fuseiter_59)]);
+    }
+  }
+  sc_thread_aligned_free(__stream, __rescheduled_1);
+}
+
+static void outerloop_8_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_60_closure_0_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
+  outerloop_8_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_60_closure_0(__stream, __module_data, i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr), (float*)(args[3UL].v_ptr), (float*)(args[4UL].v_ptr));
+}
+
diff --git a/src/fp32_matmul_softmax_fusion_100111.hpp b/src/fp32_matmul_softmax_fusion_100111.hpp
new file mode 100644
index 0000000..d0d1d69
--- /dev/null
+++ b/src/fp32_matmul_softmax_fusion_100111.hpp
@@ -0,0 +1,24 @@
+#include <stdint.h>
+#include <runtime/generic_val.hpp>
+using generic_val = sc::generic_val;
+
+extern uint8_t fp32_matmul_softmax_fusion_100111_data[8];
+
+/**
+ * fp32_matmul_softmax_fusion_100111
+ * @param __stream the stream pointer, usually get_default_stream()
+ * @param __module_data the module global data
+ * @param args The array of arguments. It should contain the following:
+ *   -param logical_tensor_125 [f32 [8, 48, 48] @ ABC]
+ *   -param logical_tensor_112 [f32 [8, 48, 32] @ ABC]
+ *   -param logical_tensor_121 [f32 [8, 48, 32] @ ABC]
+ *   -param logical_tensor_77 [f32 [1] @ A]
+ *   -param logical_tensor_97 [f32 [8, 48, 48] @ ABC]
+*/
+extern "C" void fp32_matmul_softmax_fusion_100111_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,3)));
+/**
+ * Initialize the fp32_matmul_softmax_fusion_100111
+ * @param __stream the stream pointer, usually get_default_stream()
+ * @param __module_data the module global data
+*/
+extern "C" void sc_init_fp32_matmul_softmax_fusion_100111(void* __stream, int8_t* __restrict__ __module_data) noexcept __attribute__((nonnull (2)));
diff --git a/src/fp32_matmul_softmax_fusion_100111_data.cpp b/src/fp32_matmul_softmax_fusion_100111_data.cpp
new file mode 100644
index 0000000..a52acf5
--- /dev/null
+++ b/src/fp32_matmul_softmax_fusion_100111_data.cpp
@@ -0,0 +1,3 @@
+#include <stdint.h>
+
+alignas(64) uint8_t fp32_matmul_softmax_fusion_100111_data[8] = {0,0,0,0,0,0,0,0,};
diff --git a/src/fp32_mha_pattern_alternative3_100156.cpp b/src/fp32_mha_pattern_alternative3_100156.cpp
new file mode 100644
index 0000000..134fb2a
--- /dev/null
+++ b/src/fp32_mha_pattern_alternative3_100156.cpp
@@ -0,0 +1,265 @@
+#include <runtime/kernel_include/cpu_include.hpp>
+
+#include <omp.h>
+#define sc_get_thread_id omp_get_thread_num
+#define sc_parallel_call_cpu_with_env sc_parallel_call_cpu_with_env_impl
+static bool outerloop_4088_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_7(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ matmul_core_14_outs_0, float* __restrict__ reorder_1_ins_0, float* __restrict__ matmul_core_3_ins_0, float* __restrict__ mul_5_ins_1, float* __restrict__ add_7_ins_0, float* __restrict__ matmul_core_14_ins_1) noexcept __attribute__((nonnull (2,3,4,5,6,7,8)));
+extern "C" void sc_parallel_call_cpu_with_env(void* func, uint64_t flags, void* stream, int8_t* env, uint64_t begin, uint64_t end, uint64_t step, generic_val* args) noexcept;
+static void outerloop_4088_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_70_closure_0_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
+/**
+ * fp32_mha_pattern_alternative3_100156
+ * @param __stream the stream pointer, usually get_default_stream()
+ * @param __module_data the module global data
+ * @param logical_tensor_89 [f32 [4088, 1, 32] @ ABC]
+ * @param logical_tensor_10 [f32 [4088, 1, 32] @ ABC]
+ * @param logical_tensor_75 [f32 [4088, 48, 32] @ ABC]
+ * @param logical_tensor_77 [f32 [1] @ A]
+ * @param logical_tensor_60 [f32 [4088, 1, 48] @ ABC]
+ * @param logical_tensor_88 [f32 [4088, 48, 32] @ ABC]
+*/
+static void fp32_mha_pattern_alternative3_100156(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ logical_tensor_89, float* __restrict__ logical_tensor_10, float* __restrict__ logical_tensor_75, float* __restrict__ logical_tensor_77, float* __restrict__ logical_tensor_60, float* __restrict__ logical_tensor_88) noexcept __attribute__((nonnull (2,3,4,5,6,7,8)));
+extern "C" void* dnnl_brgemm_func(int32_t M, int32_t N, int32_t K, int32_t LDA, int32_t LDB, int32_t LDC, int32_t stride_a, int32_t stride_b, float beta, int32_t dtypeA, int32_t dtypeB, void* brg_attrs, void* bd_mask, void* postops_setting) noexcept;
+extern "C" void* sc_thread_aligned_malloc(void* stream, uint64_t size) noexcept __attribute__((returns_nonnull))  __attribute__((malloc));
+extern "C" void dnnl_brgemm_call(void* func, void* A, void* B, void* C, int32_t num, void* stream) noexcept;
+extern "C" void sc_thread_aligned_free(void* stream, void* ptr) noexcept;
+static void outerloop_4088_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_70_closure_0(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0, float* __restrict__ reorder_1_ins_0, float* __restrict__ matmul_core_3_ins_0, float* __restrict__ mul_5_ins_1, float* __restrict__ add_7_ins_0, float* __restrict__ matmul_core_14_ins_1, float* __restrict__ matmul_core_14_outs_0) noexcept __attribute__((nonnull (2,4,5,6,7,8,9)));
+
+
+static void fp32_mha_pattern_alternative3_100156(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ logical_tensor_89, float* __restrict__ logical_tensor_10, float* __restrict__ logical_tensor_75, float* __restrict__ logical_tensor_77, float* __restrict__ logical_tensor_60, float* __restrict__ logical_tensor_88) noexcept{
+  outerloop_4088_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_7(__stream, __module_data, logical_tensor_89, &logical_tensor_75[0UL], logical_tensor_10, logical_tensor_77, logical_tensor_60, logical_tensor_88);
+}
+
+static bool outerloop_4088_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_7(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ matmul_core_14_outs_0, float* __restrict__ reorder_1_ins_0, float* __restrict__ matmul_core_3_ins_0, float* __restrict__ mul_5_ins_1, float* __restrict__ add_7_ins_0, float* __restrict__ matmul_core_14_ins_1) noexcept{
+  generic_val __tempargs0[6UL];
+  __tempargs0[0UL] = reorder_1_ins_0;
+  __tempargs0[1UL] = matmul_core_3_ins_0;
+  __tempargs0[2UL] = mul_5_ins_1;
+  __tempargs0[3UL] = add_7_ins_0;
+  __tempargs0[4UL] = matmul_core_14_ins_1;
+  __tempargs0[5UL] = matmul_core_14_outs_0;
+  sc_parallel_call_cpu_with_env((void*)&outerloop_4088_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_70_closure_0_0wrapper, 0UL, __stream, __module_data, 0UL, 4088UL, 1UL, __tempargs0);
+  return true;
+}
+
+extern "C" void fp32_mha_pattern_alternative3_100156_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
+  generic_val __cached_0;
+  __cached_0 = args[0UL];
+  generic_val __cached_1;
+  __cached_1 = args[1UL];
+  generic_val __cached_2;
+  __cached_2 = args[2UL];
+  generic_val __cached_3;
+  __cached_3 = args[3UL];
+  generic_val __cached_4;
+  __cached_4 = args[4UL];
+  generic_val __cached_5;
+  __cached_5 = args[5UL];
+  fp32_mha_pattern_alternative3_100156(__stream, __module_data, (float*)(__cached_0.v_ptr), (float*)(__cached_1.v_ptr), (float*)(__cached_2.v_ptr), (float*)(__cached_3.v_ptr), (float*)(__cached_4.v_ptr), (float*)(__cached_5.v_ptr));
+}
+
+static void outerloop_4088_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_7_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
+  generic_val __cached_0;
+  __cached_0 = args[0UL];
+  generic_val __cached_1;
+  __cached_1 = args[1UL];
+  generic_val __cached_2;
+  __cached_2 = args[2UL];
+  generic_val __cached_3;
+  __cached_3 = args[3UL];
+  generic_val __cached_4;
+  __cached_4 = args[4UL];
+  generic_val __cached_5;
+  __cached_5 = args[5UL];
+  outerloop_4088_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_7(__stream, __module_data, (float*)(__cached_0.v_ptr), (float*)(__cached_1.v_ptr), (float*)(__cached_2.v_ptr), (float*)(__cached_3.v_ptr), (float*)(__cached_4.v_ptr), (float*)(__cached_5.v_ptr));
+}
+
+static vec_f32x16 _should_inline_exp_f32x16(void* __stream, int8_t* __restrict__ __module_data, vec_f32x16 _intrin_v0) noexcept{
+  vec_f32x16 a_;
+  a_ = sc_min(_intrin_v0, vec_f32x16(88.5999985));
+  vec_f32x16 k_float;
+  k_float = sc_floor((a_ * vec_f32x16(1.44269502)));
+  vec_s32x16 k_int;
+  k_int = (vec_s32x16)(k_float);
+  vec_f32x16 r;
+  r = (a_ - (k_float * vec_f32x16(0.693147182)));
+  vec_f32x16 Tn;
+  Tn = vec_f32x16(1.f);
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.166666672)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.200000003)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.25)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.333333343)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.5)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(1.f)), vec_f32x16(1.f));
+  vec_s32x16 result;
+  result = ((k_int << vec_s32x16(23)) + sc_reinterpret<vec_s32x16>(Tn));
+  return sc_select((_intrin_v0 >= vec_f32x16(-87.3300018)), sc_reinterpret<vec_f32x16>(result), vec_f32x16(0.f));
+}
+
+extern "C" void sc_init_fp32_mha_pattern_alternative3_100156(void* __stream, int8_t* __restrict__ __module_data) noexcept{
+  void*& __sc_kernel_cache = *(void**)(__module_data + 0);
+  void*& __sc_kernel_cache_4 = *(void**)(__module_data + 8);
+  __sc_kernel_cache = dnnl_brgemm_func(1, 48, 32, 32, 48, 48, 32, 1536, 0.f, 4, 4, ((void*)0), ((void*)0), ((void*)0));
+  __sc_kernel_cache_4 = dnnl_brgemm_func(1, 32, 48, 48, 32, 32, 48, 1536, 0.f, 4, 4, ((void*)0), ((void*)0), ((void*)0));
+}
+
+static void outerloop_4088_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_70_closure_0(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0, float* __restrict__ reorder_1_ins_0, float* __restrict__ matmul_core_3_ins_0, float* __restrict__ mul_5_ins_1, float* __restrict__ add_7_ins_0, float* __restrict__ matmul_core_14_ins_1, float* __restrict__ matmul_core_14_outs_0) noexcept{
+  void*& __sc_kernel_cache = *(void**)(__module_data + 0);
+  void*& __sc_kernel_cache_4 = *(void**)(__module_data + 8);
+  int8_t* __rescheduled_1 = (int8_t*)sc_thread_aligned_malloc(__stream, 6336UL);
+  float* reorder_1_outs_0_shr = (float*)&__rescheduled_1[0UL];
+  float* mul_11_outs_0_shr = (float*)&__rescheduled_1[256UL];
+  float* mul_5_outs_0_shr = (float*)&__rescheduled_1[0UL];
+  float* matmul_core_3_outs_0_shr = (float*)&__rescheduled_1[6144UL];
+  vec_f32x16 reduce_compute_15_outs_0_shr0;
+  vec_f32x16 __cached_0;
+  __cached_0 = vec_f32x16(0.f);
+  reduce_compute_15_outs_0_shr0 = __cached_0;
+  float* reduce_collect_16_outs_0_shr = (float*)&__rescheduled_1[0UL];
+  float __cached_1;
+  __cached_1 = 0.f;
+  reduce_collect_16_outs_0_shr[0UL] = __cached_1;
+  float* reciprocal_10_outs_0_shr = (float*)&__rescheduled_1[192UL];
+  for (uint64_t _fuseiter_153 = 0UL; _fuseiter_153 < 32UL; _fuseiter_153 += 8UL) {
+    for (uint64_t _fuseiter_154 = 0UL; _fuseiter_154 < 48UL; _fuseiter_154 += 8UL) {
+      vec_f32x8 row1_63;
+      vec_f32x8 row2_64;
+      vec_f32x8 row3_65;
+      vec_f32x8 row4_66;
+      vec_f32x8 row5_67;
+      vec_f32x8 row6_68;
+      vec_f32x8 row7_69;
+      vec_f32x8 row8_70;
+      vec_f32x8 row9_71;
+      vec_f32x8 row10_72;
+      vec_f32x8 row11_73;
+      vec_f32x8 row12_74;
+      vec_f32x8 __cached_2;
+      __cached_2 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + ((_fuseiter_154 * 32UL) + _fuseiter_153))]);
+      row1_63 = __cached_2;
+      vec_f32x8 __cached_3;
+      __cached_3 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_154 + 1UL) * 32UL) + _fuseiter_153))]);
+      row2_64 = __cached_3;
+      vec_f32x8 __cached_4;
+      __cached_4 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_154 + 2UL) * 32UL) + _fuseiter_153))]);
+      row3_65 = __cached_4;
+      vec_f32x8 __cached_5;
+      __cached_5 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_154 + 3UL) * 32UL) + _fuseiter_153))]);
+      row4_66 = __cached_5;
+      vec_f32x8 __cached_6;
+      __cached_6 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_154 + 4UL) * 32UL) + _fuseiter_153))]);
+      row5_67 = __cached_6;
+      vec_f32x8 __cached_7;
+      __cached_7 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_154 + 5UL) * 32UL) + _fuseiter_153))]);
+      row6_68 = __cached_7;
+      vec_f32x8 __cached_8;
+      __cached_8 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_154 + 6UL) * 32UL) + _fuseiter_153))]);
+      row7_69 = __cached_8;
+      vec_f32x8 __cached_9;
+      __cached_9 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_154 + 7UL) * 32UL) + _fuseiter_153))]);
+      row8_70 = __cached_9;
+      row9_71 = sc_unpack_low(row1_63, row2_64, 32);
+      row1_63 = sc_unpack_high(row1_63, row2_64, 32);
+      row10_72 = sc_unpack_low(row3_65, row4_66, 32);
+      row2_64 = sc_unpack_high(row3_65, row4_66, 32);
+      row11_73 = sc_unpack_low(row5_67, row6_68, 32);
+      row3_65 = sc_unpack_high(row5_67, row6_68, 32);
+      row12_74 = sc_unpack_low(row7_69, row8_70, 32);
+      row4_66 = sc_unpack_high(row7_69, row8_70, 32);
+      row5_67 = sc_shuffle(row9_71, row10_72, 68);
+      row6_68 = sc_shuffle(row9_71, row10_72, 238);
+      row7_69 = sc_shuffle(row1_63, row2_64, 68);
+      row8_70 = sc_shuffle(row1_63, row2_64, 238);
+      row9_71 = sc_shuffle(row11_73, row12_74, 68);
+      row10_72 = sc_shuffle(row11_73, row12_74, 238);
+      row11_73 = sc_shuffle(row3_65, row4_66, 68);
+      row12_74 = sc_shuffle(row3_65, row4_66, 238);
+      row1_63 = sc_permute(row5_67, row9_71, 32);
+      row2_64 = sc_permute(row6_68, row10_72, 32);
+      row3_65 = sc_permute(row7_69, row11_73, 32);
+      row4_66 = sc_permute(row8_70, row12_74, 32);
+      row5_67 = sc_permute(row5_67, row9_71, 49);
+      row6_68 = sc_permute(row6_68, row10_72, 49);
+      row7_69 = sc_permute(row7_69, row11_73, 49);
+      row8_70 = sc_permute(row8_70, row12_74, 49);
+      vec_f32x8 __cached_10;
+      __cached_10 = row1_63;
+      vec_f32x8::store(__cached_10, &reorder_1_outs_0_shr[((_fuseiter_153 * 48UL) + _fuseiter_154)]);
+      vec_f32x8 __cached_11;
+      __cached_11 = row2_64;
+      vec_f32x8::store(__cached_11, &reorder_1_outs_0_shr[(((_fuseiter_153 + 1UL) * 48UL) + _fuseiter_154)]);
+      vec_f32x8 __cached_12;
+      __cached_12 = row3_65;
+      vec_f32x8::store(__cached_12, &reorder_1_outs_0_shr[(((_fuseiter_153 + 2UL) * 48UL) + _fuseiter_154)]);
+      vec_f32x8 __cached_13;
+      __cached_13 = row4_66;
+      vec_f32x8::store(__cached_13, &reorder_1_outs_0_shr[(((_fuseiter_153 + 3UL) * 48UL) + _fuseiter_154)]);
+      vec_f32x8 __cached_14;
+      __cached_14 = row5_67;
+      vec_f32x8::store(__cached_14, &reorder_1_outs_0_shr[(((_fuseiter_153 + 4UL) * 48UL) + _fuseiter_154)]);
+      vec_f32x8 __cached_15;
+      __cached_15 = row6_68;
+      vec_f32x8::store(__cached_15, &reorder_1_outs_0_shr[(((_fuseiter_153 + 5UL) * 48UL) + _fuseiter_154)]);
+      vec_f32x8 __cached_16;
+      __cached_16 = row7_69;
+      vec_f32x8::store(__cached_16, &reorder_1_outs_0_shr[(((_fuseiter_153 + 6UL) * 48UL) + _fuseiter_154)]);
+      vec_f32x8 __cached_17;
+      __cached_17 = row8_70;
+      vec_f32x8::store(__cached_17, &reorder_1_outs_0_shr[(((_fuseiter_153 + 7UL) * 48UL) + _fuseiter_154)]);
+    }
+  }
+  dnnl_brgemm_call(__sc_kernel_cache, &matmul_core_3_ins_0[(__itr_0 * 32UL)], &reorder_1_outs_0_shr[0UL], &matmul_core_3_outs_0_shr[0UL], 1, __stream);
+  for (uint64_t _fuseiter_157 = 0UL; _fuseiter_157 < 48UL; _fuseiter_157 += 16UL) {
+    vec_f32x16 __cached_18;
+    __cached_18 = vec_f32x16::load(&matmul_core_3_outs_0_shr[_fuseiter_157]);
+    vec_f32x16 __cached_19;
+    __cached_19 = (__cached_18 * vec_f32x16(mul_5_ins_1[0]));
+    vec_f32x16 __cached_20;
+    __cached_20 = vec_f32x16::load(&add_7_ins_0[((__itr_0 * 48UL) + _fuseiter_157)]);
+    __cached_19 = (__cached_20 + __cached_19);
+    vec_f32x16 _retval5;
+    vec_f32x16 a_;
+    a_ = sc_min(__cached_19, vec_f32x16(88.5999985));
+    vec_f32x16 k_float;
+    k_float = sc_floor((a_ * vec_f32x16(1.44269502)));
+    vec_s32x16 k_int;
+    k_int = (vec_s32x16)(k_float);
+    vec_f32x16 r;
+    r = (a_ - (k_float * vec_f32x16(0.693147182)));
+    vec_f32x16 Tn;
+    Tn = vec_f32x16(1.f);
+    Tn = sc_fmadd(Tn, (r * vec_f32x16(0.166666672)), vec_f32x16(1.f));
+    Tn = sc_fmadd(Tn, (r * vec_f32x16(0.200000003)), vec_f32x16(1.f));
+    Tn = sc_fmadd(Tn, (r * vec_f32x16(0.25)), vec_f32x16(1.f));
+    Tn = sc_fmadd(Tn, (r * vec_f32x16(0.333333343)), vec_f32x16(1.f));
+    Tn = sc_fmadd(Tn, (r * vec_f32x16(0.5)), vec_f32x16(1.f));
+    Tn = sc_fmadd(Tn, (r * vec_f32x16(1.f)), vec_f32x16(1.f));
+    vec_s32x16 result;
+    result = ((k_int << vec_s32x16(23)) + sc_reinterpret<vec_s32x16>(Tn));
+    _retval5 = sc_select((__cached_19 >= vec_f32x16(-87.3300018)), sc_reinterpret<vec_f32x16>(result), vec_f32x16(0.f));
+    __cached_19 = _retval5;
+    vec_f32x16::store(__cached_19, &mul_5_outs_0_shr[_fuseiter_157]);
+    vec_f32x16 __cached_21;
+    __cached_21 = reduce_compute_15_outs_0_shr0;
+    __cached_21 = (__cached_21 + __cached_19);
+    reduce_compute_15_outs_0_shr0 = __cached_21;
+  }
+  vec_f32x16 __cached_22;
+  __cached_22 = reduce_compute_15_outs_0_shr0;
+  float __cached_23;
+  __cached_23 = sc_reduce_add(__cached_22);
+  reciprocal_10_outs_0_shr[0UL] = (1.f / __cached_23);
+  for (uint64_t _fuseiter_181 = 0UL; _fuseiter_181 < 48UL; _fuseiter_181 += 16UL) {
+    vec_f32x16 __cached_24;
+    __cached_24 = vec_f32x16::load(&mul_5_outs_0_shr[_fuseiter_181]);
+    vec_f32x16 __cached_25;
+    __cached_25 = (__cached_24 * vec_f32x16(reciprocal_10_outs_0_shr[0UL]));
+    vec_f32x16::store(__cached_25, &mul_11_outs_0_shr[_fuseiter_181]);
+  }
+  dnnl_brgemm_call(__sc_kernel_cache_4, &mul_11_outs_0_shr[0UL], &matmul_core_14_ins_1[(__itr_0 * 1536UL)], &matmul_core_14_outs_0[(__itr_0 * 32UL)], 1, __stream);
+  sc_thread_aligned_free(__stream, __rescheduled_1);
+}
+
+static void outerloop_4088_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_70_closure_0_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
+  outerloop_4088_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_70_closure_0(__stream, __module_data, i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr), (float*)(args[3UL].v_ptr), (float*)(args[4UL].v_ptr), (float*)(args[5UL].v_ptr));
+}
+
diff --git a/src/fp32_mha_pattern_alternative3_100156.hpp b/src/fp32_mha_pattern_alternative3_100156.hpp
new file mode 100644
index 0000000..332abb2
--- /dev/null
+++ b/src/fp32_mha_pattern_alternative3_100156.hpp
@@ -0,0 +1,25 @@
+#include <stdint.h>
+#include <runtime/generic_val.hpp>
+using generic_val = sc::generic_val;
+
+extern uint8_t fp32_mha_pattern_alternative3_100156_data[16];
+
+/**
+ * fp32_mha_pattern_alternative3_100156
+ * @param __stream the stream pointer, usually get_default_stream()
+ * @param __module_data the module global data
+ * @param args The array of arguments. It should contain the following:
+ *   -param logical_tensor_89 [f32 [4088, 1, 32] @ ABC]
+ *   -param logical_tensor_10 [f32 [4088, 1, 32] @ ABC]
+ *   -param logical_tensor_75 [f32 [4088, 48, 32] @ ABC]
+ *   -param logical_tensor_77 [f32 [1] @ A]
+ *   -param logical_tensor_60 [f32 [4088, 1, 48] @ ABC]
+ *   -param logical_tensor_88 [f32 [4088, 48, 32] @ ABC]
+*/
+extern "C" void fp32_mha_pattern_alternative3_100156_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,3)));
+/**
+ * Initialize the fp32_mha_pattern_alternative3_100156
+ * @param __stream the stream pointer, usually get_default_stream()
+ * @param __module_data the module global data
+*/
+extern "C" void sc_init_fp32_mha_pattern_alternative3_100156(void* __stream, int8_t* __restrict__ __module_data) noexcept __attribute__((nonnull (2)));
diff --git a/src/fp32_mha_pattern_alternative3_100156_data.cpp b/src/fp32_mha_pattern_alternative3_100156_data.cpp
new file mode 100644
index 0000000..c27841e
--- /dev/null
+++ b/src/fp32_mha_pattern_alternative3_100156_data.cpp
@@ -0,0 +1,3 @@
+#include <stdint.h>
+
+alignas(64) uint8_t fp32_mha_pattern_alternative3_100156_data[16] = {0,1,0,136,158,127,0,0,0,173,73,136,158,127,0,0,};
diff --git a/src/fp32_mha_pattern_alternative3_100183.cpp b/src/fp32_mha_pattern_alternative3_100183.cpp
new file mode 100644
index 0000000..6ccda39
--- /dev/null
+++ b/src/fp32_mha_pattern_alternative3_100183.cpp
@@ -0,0 +1,265 @@
+#include <runtime/kernel_include/cpu_include.hpp>
+
+#include <omp.h>
+#define sc_get_thread_id omp_get_thread_num
+#define sc_parallel_call_cpu_with_env sc_parallel_call_cpu_with_env_impl
+static bool outerloop_8176_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_7(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ matmul_core_14_outs_0, float* __restrict__ reorder_1_ins_0, float* __restrict__ matmul_core_3_ins_0, float* __restrict__ mul_5_ins_1, float* __restrict__ add_7_ins_0, float* __restrict__ matmul_core_14_ins_1) noexcept __attribute__((nonnull (2,3,4,5,6,7,8)));
+extern "C" void sc_parallel_call_cpu_with_env(void* func, uint64_t flags, void* stream, int8_t* env, uint64_t begin, uint64_t end, uint64_t step, generic_val* args) noexcept;
+static void outerloop_8176_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_70_closure_0_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
+/**
+ * fp32_mha_pattern_alternative3_100183
+ * @param __stream the stream pointer, usually get_default_stream()
+ * @param __module_data the module global data
+ * @param logical_tensor_159 [f32 [8176, 1, 32] @ ABC]
+ * @param logical_tensor_21 [f32 [8176, 1, 32] @ ABC]
+ * @param logical_tensor_148 [f32 [8176, 48, 32] @ ABC]
+ * @param logical_tensor_77 [f32 [1] @ A]
+ * @param logical_tensor_95 [f32 [8176, 1, 48] @ ABC]
+ * @param logical_tensor_158 [f32 [8176, 48, 32] @ ABC]
+*/
+static void fp32_mha_pattern_alternative3_100183(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ logical_tensor_159, float* __restrict__ logical_tensor_21, float* __restrict__ logical_tensor_148, float* __restrict__ logical_tensor_77, float* __restrict__ logical_tensor_95, float* __restrict__ logical_tensor_158) noexcept __attribute__((nonnull (2,3,4,5,6,7,8)));
+extern "C" void* dnnl_brgemm_func(int32_t M, int32_t N, int32_t K, int32_t LDA, int32_t LDB, int32_t LDC, int32_t stride_a, int32_t stride_b, float beta, int32_t dtypeA, int32_t dtypeB, void* brg_attrs, void* bd_mask, void* postops_setting) noexcept;
+extern "C" void* sc_thread_aligned_malloc(void* stream, uint64_t size) noexcept __attribute__((returns_nonnull))  __attribute__((malloc));
+extern "C" void dnnl_brgemm_call(void* func, void* A, void* B, void* C, int32_t num, void* stream) noexcept;
+extern "C" void sc_thread_aligned_free(void* stream, void* ptr) noexcept;
+static void outerloop_8176_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_70_closure_0(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0, float* __restrict__ reorder_1_ins_0, float* __restrict__ matmul_core_3_ins_0, float* __restrict__ mul_5_ins_1, float* __restrict__ add_7_ins_0, float* __restrict__ matmul_core_14_ins_1, float* __restrict__ matmul_core_14_outs_0) noexcept __attribute__((nonnull (2,4,5,6,7,8,9)));
+
+
+static void fp32_mha_pattern_alternative3_100183(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ logical_tensor_159, float* __restrict__ logical_tensor_21, float* __restrict__ logical_tensor_148, float* __restrict__ logical_tensor_77, float* __restrict__ logical_tensor_95, float* __restrict__ logical_tensor_158) noexcept{
+  outerloop_8176_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_7(__stream, __module_data, logical_tensor_159, &logical_tensor_148[0UL], logical_tensor_21, logical_tensor_77, logical_tensor_95, logical_tensor_158);
+}
+
+static bool outerloop_8176_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_7(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ matmul_core_14_outs_0, float* __restrict__ reorder_1_ins_0, float* __restrict__ matmul_core_3_ins_0, float* __restrict__ mul_5_ins_1, float* __restrict__ add_7_ins_0, float* __restrict__ matmul_core_14_ins_1) noexcept{
+  generic_val __tempargs0[6UL];
+  __tempargs0[0UL] = reorder_1_ins_0;
+  __tempargs0[1UL] = matmul_core_3_ins_0;
+  __tempargs0[2UL] = mul_5_ins_1;
+  __tempargs0[3UL] = add_7_ins_0;
+  __tempargs0[4UL] = matmul_core_14_ins_1;
+  __tempargs0[5UL] = matmul_core_14_outs_0;
+  sc_parallel_call_cpu_with_env((void*)&outerloop_8176_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_70_closure_0_0wrapper, 0UL, __stream, __module_data, 0UL, 8176UL, 1UL, __tempargs0);
+  return true;
+}
+
+extern "C" void fp32_mha_pattern_alternative3_100183_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
+  generic_val __cached_0;
+  __cached_0 = args[0UL];
+  generic_val __cached_1;
+  __cached_1 = args[1UL];
+  generic_val __cached_2;
+  __cached_2 = args[2UL];
+  generic_val __cached_3;
+  __cached_3 = args[3UL];
+  generic_val __cached_4;
+  __cached_4 = args[4UL];
+  generic_val __cached_5;
+  __cached_5 = args[5UL];
+  fp32_mha_pattern_alternative3_100183(__stream, __module_data, (float*)(__cached_0.v_ptr), (float*)(__cached_1.v_ptr), (float*)(__cached_2.v_ptr), (float*)(__cached_3.v_ptr), (float*)(__cached_4.v_ptr), (float*)(__cached_5.v_ptr));
+}
+
+static void outerloop_8176_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_7_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
+  generic_val __cached_0;
+  __cached_0 = args[0UL];
+  generic_val __cached_1;
+  __cached_1 = args[1UL];
+  generic_val __cached_2;
+  __cached_2 = args[2UL];
+  generic_val __cached_3;
+  __cached_3 = args[3UL];
+  generic_val __cached_4;
+  __cached_4 = args[4UL];
+  generic_val __cached_5;
+  __cached_5 = args[5UL];
+  outerloop_8176_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_7(__stream, __module_data, (float*)(__cached_0.v_ptr), (float*)(__cached_1.v_ptr), (float*)(__cached_2.v_ptr), (float*)(__cached_3.v_ptr), (float*)(__cached_4.v_ptr), (float*)(__cached_5.v_ptr));
+}
+
+static vec_f32x16 _should_inline_exp_f32x16(void* __stream, int8_t* __restrict__ __module_data, vec_f32x16 _intrin_v0) noexcept{
+  vec_f32x16 a_;
+  a_ = sc_min(_intrin_v0, vec_f32x16(88.5999985));
+  vec_f32x16 k_float;
+  k_float = sc_floor((a_ * vec_f32x16(1.44269502)));
+  vec_s32x16 k_int;
+  k_int = (vec_s32x16)(k_float);
+  vec_f32x16 r;
+  r = (a_ - (k_float * vec_f32x16(0.693147182)));
+  vec_f32x16 Tn;
+  Tn = vec_f32x16(1.f);
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.166666672)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.200000003)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.25)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.333333343)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(0.5)), vec_f32x16(1.f));
+  Tn = sc_fmadd(Tn, (r * vec_f32x16(1.f)), vec_f32x16(1.f));
+  vec_s32x16 result;
+  result = ((k_int << vec_s32x16(23)) + sc_reinterpret<vec_s32x16>(Tn));
+  return sc_select((_intrin_v0 >= vec_f32x16(-87.3300018)), sc_reinterpret<vec_f32x16>(result), vec_f32x16(0.f));
+}
+
+extern "C" void sc_init_fp32_mha_pattern_alternative3_100183(void* __stream, int8_t* __restrict__ __module_data) noexcept{
+  void*& __sc_kernel_cache = *(void**)(__module_data + 0);
+  void*& __sc_kernel_cache_2 = *(void**)(__module_data + 8);
+  __sc_kernel_cache = dnnl_brgemm_func(1, 48, 32, 32, 48, 48, 32, 1536, 0.f, 4, 4, ((void*)0), ((void*)0), ((void*)0));
+  __sc_kernel_cache_2 = dnnl_brgemm_func(1, 32, 48, 48, 32, 32, 48, 1536, 0.f, 4, 4, ((void*)0), ((void*)0), ((void*)0));
+}
+
+static void outerloop_8176_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_70_closure_0(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0, float* __restrict__ reorder_1_ins_0, float* __restrict__ matmul_core_3_ins_0, float* __restrict__ mul_5_ins_1, float* __restrict__ add_7_ins_0, float* __restrict__ matmul_core_14_ins_1, float* __restrict__ matmul_core_14_outs_0) noexcept{
+  void*& __sc_kernel_cache = *(void**)(__module_data + 0);
+  void*& __sc_kernel_cache_2 = *(void**)(__module_data + 8);
+  int8_t* __rescheduled_1 = (int8_t*)sc_thread_aligned_malloc(__stream, 6336UL);
+  float* reorder_1_outs_0_shr = (float*)&__rescheduled_1[0UL];
+  float* mul_11_outs_0_shr = (float*)&__rescheduled_1[256UL];
+  float* mul_5_outs_0_shr = (float*)&__rescheduled_1[0UL];
+  float* matmul_core_3_outs_0_shr = (float*)&__rescheduled_1[6144UL];
+  vec_f32x16 reduce_compute_15_outs_0_shr0;
+  vec_f32x16 __cached_0;
+  __cached_0 = vec_f32x16(0.f);
+  reduce_compute_15_outs_0_shr0 = __cached_0;
+  float* reduce_collect_16_outs_0_shr = (float*)&__rescheduled_1[0UL];
+  float __cached_1;
+  __cached_1 = 0.f;
+  reduce_collect_16_outs_0_shr[0UL] = __cached_1;
+  float* reciprocal_10_outs_0_shr = (float*)&__rescheduled_1[192UL];
+  for (uint64_t _fuseiter_92 = 0UL; _fuseiter_92 < 32UL; _fuseiter_92 += 8UL) {
+    for (uint64_t _fuseiter_93 = 0UL; _fuseiter_93 < 48UL; _fuseiter_93 += 8UL) {
+      vec_f32x8 row1_38;
+      vec_f32x8 row2_39;
+      vec_f32x8 row3_40;
+      vec_f32x8 row4_41;
+      vec_f32x8 row5_42;
+      vec_f32x8 row6_43;
+      vec_f32x8 row7_44;
+      vec_f32x8 row8_45;
+      vec_f32x8 row9_46;
+      vec_f32x8 row10_47;
+      vec_f32x8 row11_48;
+      vec_f32x8 row12_49;
+      vec_f32x8 __cached_2;
+      __cached_2 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + ((_fuseiter_93 * 32UL) + _fuseiter_92))]);
+      row1_38 = __cached_2;
+      vec_f32x8 __cached_3;
+      __cached_3 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_93 + 1UL) * 32UL) + _fuseiter_92))]);
+      row2_39 = __cached_3;
+      vec_f32x8 __cached_4;
+      __cached_4 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_93 + 2UL) * 32UL) + _fuseiter_92))]);
+      row3_40 = __cached_4;
+      vec_f32x8 __cached_5;
+      __cached_5 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_93 + 3UL) * 32UL) + _fuseiter_92))]);
+      row4_41 = __cached_5;
+      vec_f32x8 __cached_6;
+      __cached_6 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_93 + 4UL) * 32UL) + _fuseiter_92))]);
+      row5_42 = __cached_6;
+      vec_f32x8 __cached_7;
+      __cached_7 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_93 + 5UL) * 32UL) + _fuseiter_92))]);
+      row6_43 = __cached_7;
+      vec_f32x8 __cached_8;
+      __cached_8 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_93 + 6UL) * 32UL) + _fuseiter_92))]);
+      row7_44 = __cached_8;
+      vec_f32x8 __cached_9;
+      __cached_9 = vec_f32x8::load(&reorder_1_ins_0[((__itr_0 * 1536UL) + (((_fuseiter_93 + 7UL) * 32UL) + _fuseiter_92))]);
+      row8_45 = __cached_9;
+      row9_46 = sc_unpack_low(row1_38, row2_39, 32);
+      row1_38 = sc_unpack_high(row1_38, row2_39, 32);
+      row10_47 = sc_unpack_low(row3_40, row4_41, 32);
+      row2_39 = sc_unpack_high(row3_40, row4_41, 32);
+      row11_48 = sc_unpack_low(row5_42, row6_43, 32);
+      row3_40 = sc_unpack_high(row5_42, row6_43, 32);
+      row12_49 = sc_unpack_low(row7_44, row8_45, 32);
+      row4_41 = sc_unpack_high(row7_44, row8_45, 32);
+      row5_42 = sc_shuffle(row9_46, row10_47, 68);
+      row6_43 = sc_shuffle(row9_46, row10_47, 238);
+      row7_44 = sc_shuffle(row1_38, row2_39, 68);
+      row8_45 = sc_shuffle(row1_38, row2_39, 238);
+      row9_46 = sc_shuffle(row11_48, row12_49, 68);
+      row10_47 = sc_shuffle(row11_48, row12_49, 238);
+      row11_48 = sc_shuffle(row3_40, row4_41, 68);
+      row12_49 = sc_shuffle(row3_40, row4_41, 238);
+      row1_38 = sc_permute(row5_42, row9_46, 32);
+      row2_39 = sc_permute(row6_43, row10_47, 32);
+      row3_40 = sc_permute(row7_44, row11_48, 32);
+      row4_41 = sc_permute(row8_45, row12_49, 32);
+      row5_42 = sc_permute(row5_42, row9_46, 49);
+      row6_43 = sc_permute(row6_43, row10_47, 49);
+      row7_44 = sc_permute(row7_44, row11_48, 49);
+      row8_45 = sc_permute(row8_45, row12_49, 49);
+      vec_f32x8 __cached_10;
+      __cached_10 = row1_38;
+      vec_f32x8::store(__cached_10, &reorder_1_outs_0_shr[((_fuseiter_92 * 48UL) + _fuseiter_93)]);
+      vec_f32x8 __cached_11;
+      __cached_11 = row2_39;
+      vec_f32x8::store(__cached_11, &reorder_1_outs_0_shr[(((_fuseiter_92 + 1UL) * 48UL) + _fuseiter_93)]);
+      vec_f32x8 __cached_12;
+      __cached_12 = row3_40;
+      vec_f32x8::store(__cached_12, &reorder_1_outs_0_shr[(((_fuseiter_92 + 2UL) * 48UL) + _fuseiter_93)]);
+      vec_f32x8 __cached_13;
+      __cached_13 = row4_41;
+      vec_f32x8::store(__cached_13, &reorder_1_outs_0_shr[(((_fuseiter_92 + 3UL) * 48UL) + _fuseiter_93)]);
+      vec_f32x8 __cached_14;
+      __cached_14 = row5_42;
+      vec_f32x8::store(__cached_14, &reorder_1_outs_0_shr[(((_fuseiter_92 + 4UL) * 48UL) + _fuseiter_93)]);
+      vec_f32x8 __cached_15;
+      __cached_15 = row6_43;
+      vec_f32x8::store(__cached_15, &reorder_1_outs_0_shr[(((_fuseiter_92 + 5UL) * 48UL) + _fuseiter_93)]);
+      vec_f32x8 __cached_16;
+      __cached_16 = row7_44;
+      vec_f32x8::store(__cached_16, &reorder_1_outs_0_shr[(((_fuseiter_92 + 6UL) * 48UL) + _fuseiter_93)]);
+      vec_f32x8 __cached_17;
+      __cached_17 = row8_45;
+      vec_f32x8::store(__cached_17, &reorder_1_outs_0_shr[(((_fuseiter_92 + 7UL) * 48UL) + _fuseiter_93)]);
+    }
+  }
+  dnnl_brgemm_call(__sc_kernel_cache, &matmul_core_3_ins_0[(__itr_0 * 32UL)], &reorder_1_outs_0_shr[0UL], &matmul_core_3_outs_0_shr[0UL], 1, __stream);
+  for (uint64_t _fuseiter_96 = 0UL; _fuseiter_96 < 48UL; _fuseiter_96 += 16UL) {
+    vec_f32x16 __cached_18;
+    __cached_18 = vec_f32x16::load(&matmul_core_3_outs_0_shr[_fuseiter_96]);
+    vec_f32x16 __cached_19;
+    __cached_19 = (__cached_18 * vec_f32x16(mul_5_ins_1[0]));
+    vec_f32x16 __cached_20;
+    __cached_20 = vec_f32x16::load(&add_7_ins_0[((__itr_0 * 48UL) + _fuseiter_96)]);
+    __cached_19 = (__cached_20 + __cached_19);
+    vec_f32x16 _retval3;
+    vec_f32x16 a_;
+    a_ = sc_min(__cached_19, vec_f32x16(88.5999985));
+    vec_f32x16 k_float;
+    k_float = sc_floor((a_ * vec_f32x16(1.44269502)));
+    vec_s32x16 k_int;
+    k_int = (vec_s32x16)(k_float);
+    vec_f32x16 r;
+    r = (a_ - (k_float * vec_f32x16(0.693147182)));
+    vec_f32x16 Tn;
+    Tn = vec_f32x16(1.f);
+    Tn = sc_fmadd(Tn, (r * vec_f32x16(0.166666672)), vec_f32x16(1.f));
+    Tn = sc_fmadd(Tn, (r * vec_f32x16(0.200000003)), vec_f32x16(1.f));
+    Tn = sc_fmadd(Tn, (r * vec_f32x16(0.25)), vec_f32x16(1.f));
+    Tn = sc_fmadd(Tn, (r * vec_f32x16(0.333333343)), vec_f32x16(1.f));
+    Tn = sc_fmadd(Tn, (r * vec_f32x16(0.5)), vec_f32x16(1.f));
+    Tn = sc_fmadd(Tn, (r * vec_f32x16(1.f)), vec_f32x16(1.f));
+    vec_s32x16 result;
+    result = ((k_int << vec_s32x16(23)) + sc_reinterpret<vec_s32x16>(Tn));
+    _retval3 = sc_select((__cached_19 >= vec_f32x16(-87.3300018)), sc_reinterpret<vec_f32x16>(result), vec_f32x16(0.f));
+    __cached_19 = _retval3;
+    vec_f32x16::store(__cached_19, &mul_5_outs_0_shr[_fuseiter_96]);
+    vec_f32x16 __cached_21;
+    __cached_21 = reduce_compute_15_outs_0_shr0;
+    __cached_21 = (__cached_21 + __cached_19);
+    reduce_compute_15_outs_0_shr0 = __cached_21;
+  }
+  vec_f32x16 __cached_22;
+  __cached_22 = reduce_compute_15_outs_0_shr0;
+  float __cached_23;
+  __cached_23 = sc_reduce_add(__cached_22);
+  reciprocal_10_outs_0_shr[0UL] = (1.f / __cached_23);
+  for (uint64_t _fuseiter_120 = 0UL; _fuseiter_120 < 48UL; _fuseiter_120 += 16UL) {
+    vec_f32x16 __cached_24;
+    __cached_24 = vec_f32x16::load(&mul_5_outs_0_shr[_fuseiter_120]);
+    vec_f32x16 __cached_25;
+    __cached_25 = (__cached_24 * vec_f32x16(reciprocal_10_outs_0_shr[0UL]));
+    vec_f32x16::store(__cached_25, &mul_11_outs_0_shr[_fuseiter_120]);
+  }
+  dnnl_brgemm_call(__sc_kernel_cache_2, &mul_11_outs_0_shr[0UL], &matmul_core_14_ins_1[(__itr_0 * 1536UL)], &matmul_core_14_outs_0[(__itr_0 * 32UL)], 1, __stream);
+  sc_thread_aligned_free(__stream, __rescheduled_1);
+}
+
+static void outerloop_8176_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_70_closure_0_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
+  outerloop_8176_partition_reorder_matmul_core_mul_add_exp_reduce_compute_reduce_collect_reciprocal_mul_matmul_core_70_closure_0(__stream, __module_data, i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr), (float*)(args[3UL].v_ptr), (float*)(args[4UL].v_ptr), (float*)(args[5UL].v_ptr));
+}
+
diff --git a/src/fp32_mha_pattern_alternative3_100183.hpp b/src/fp32_mha_pattern_alternative3_100183.hpp
new file mode 100644
index 0000000..c3dd9bd
--- /dev/null
+++ b/src/fp32_mha_pattern_alternative3_100183.hpp
@@ -0,0 +1,25 @@
+#include <stdint.h>
+#include <runtime/generic_val.hpp>
+using generic_val = sc::generic_val;
+
+extern uint8_t fp32_mha_pattern_alternative3_100183_data[16];
+
+/**
+ * fp32_mha_pattern_alternative3_100183
+ * @param __stream the stream pointer, usually get_default_stream()
+ * @param __module_data the module global data
+ * @param args The array of arguments. It should contain the following:
+ *   -param logical_tensor_159 [f32 [8176, 1, 32] @ ABC]
+ *   -param logical_tensor_21 [f32 [8176, 1, 32] @ ABC]
+ *   -param logical_tensor_148 [f32 [8176, 48, 32] @ ABC]
+ *   -param logical_tensor_77 [f32 [1] @ A]
+ *   -param logical_tensor_95 [f32 [8176, 1, 48] @ ABC]
+ *   -param logical_tensor_158 [f32 [8176, 48, 32] @ ABC]
+*/
+extern "C" void fp32_mha_pattern_alternative3_100183_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,3)));
+/**
+ * Initialize the fp32_mha_pattern_alternative3_100183
+ * @param __stream the stream pointer, usually get_default_stream()
+ * @param __module_data the module global data
+*/
+extern "C" void sc_init_fp32_mha_pattern_alternative3_100183(void* __stream, int8_t* __restrict__ __module_data) noexcept __attribute__((nonnull (2)));
diff --git a/src/fp32_mha_pattern_alternative3_100183_data.cpp b/src/fp32_mha_pattern_alternative3_100183_data.cpp
new file mode 100644
index 0000000..58ba766
--- /dev/null
+++ b/src/fp32_mha_pattern_alternative3_100183_data.cpp
@@ -0,0 +1,3 @@
+#include <stdint.h>
+
+alignas(64) uint8_t fp32_mha_pattern_alternative3_100183_data[16] = {0,1,0,136,158,127,0,0,192,26,90,136,158,127,0,0,};
diff --git a/src/main.cpp b/src/main.cpp
index 1427153..7666b79 100644
--- a/src/main.cpp
+++ b/src/main.cpp
@@ -1,13 +1,52 @@
-#include "mlp.hpp"
+#include "fp32_matmul_softmax_fusion_100111.hpp"
+#include "fp32_mha_pattern_alternative3_100156.hpp"
+#include "fp32_mha_pattern_alternative3_100183.hpp"
+#include <cstring>
+#include <iostream>
 #include <runtime/context.hpp>
+#include <string>
 #include <vector>
 
-int main()
-{
-    sc_init_mlp(&sc::runtime::default_stream, (int8_t *)mlp_data);
-    std::vector<uint8_t> v1(1024*1024),v2(1024*1024),v3(4*1024*1024),v4(4*1024*1024);
-    generic_val args[] {v1.data(),v2.data(),v3.data(),v4.data()};
-    mlp_0wrapper(&sc::runtime::default_stream, (int8_t *)mlp_data, args);
-    printf("Done\n");
-    return 0;
+int main() {
+  printf("Start excute kernel 1\n");
+  sc_init_fp32_matmul_softmax_fusion_100111(
+      &sc::runtime::default_stream,
+      (int8_t *)fp32_matmul_softmax_fusion_100111_data);
+  std::vector<uint8_t> v1(8 * 48 * 48 * 4), v2(8 * 48 * 32 * 4),
+      v3(8 * 48 * 32 * 4), v4(1 * 4), v5(8 * 48 * 48 * 4);
+  generic_val args_kenrel1[]{v1.data(), v2.data(), v3.data(), v4.data(),
+                             v5.data()};
+  fp32_matmul_softmax_fusion_100111_0wrapper(
+      &sc::runtime::default_stream,
+      (int8_t *)fp32_matmul_softmax_fusion_100111_data, args_kenrel1);
+  printf("Done\n");
+
+  printf("Start excute kernel 2\n");
+  sc_init_fp32_mha_pattern_alternative3_100156(
+      &sc::runtime::default_stream,
+      (int8_t *)fp32_mha_pattern_alternative3_100156_data);
+  std::vector<uint8_t> v6(4088 * 1 * 32 * 4), v7(4088 * 1 * 32 * 4),
+      v8(4088 * 48 * 32 * 4), v9(1 * 4), v10(4088 * 1 * 48 * 4),
+      v11(4088 * 48 * 32 * 4);
+  generic_val args_kernel2[]{v6.data(), v7.data(),  v8.data(),
+                             v9.data(), v10.data(), v11.data()};
+  fp32_mha_pattern_alternative3_100156_0wrapper(
+      &sc::runtime::default_stream,
+      (int8_t *)fp32_mha_pattern_alternative3_100156_data, args_kernel2);
+  printf("Done\n");
+
+  printf("Start excute kernel 3\n");
+  sc_init_fp32_mha_pattern_alternative3_100183(
+      &sc::runtime::default_stream,
+      (int8_t *)fp32_mha_pattern_alternative3_100183_data);
+  std::vector<uint8_t> v12(8176 * 1 * 32 * 4), v13(8176 * 1 * 32 * 4),
+      v14(8176 * 48 * 32 * 4), v15(1 * 4), v16(8176 * 1 * 48 * 4),
+      v17(8176 * 48 * 32 * 4);
+  generic_val args_kernel3[]{v12.data(), v13.data(), v14.data(),
+                             v15.data(), v16.data(), v17.data()};
+  fp32_mha_pattern_alternative3_100183_0wrapper(
+      &sc::runtime::default_stream,
+      (int8_t *)fp32_mha_pattern_alternative3_100183_data, args_kernel3);
+  printf("Done\n");
+  return 0;
 }
\ No newline at end of file
diff --git a/src/mlp.cpp b/src/mlp.cpp
deleted file mode 100644
index 3c5ee78..0000000
--- a/src/mlp.cpp
+++ /dev/null
@@ -1,1032 +0,0 @@
-#include <runtime/kernel_include/cpu_include.hpp>
-
-#include <runtime/managed_thread_pool.hpp>
-#include <omp.h>
-#define sc_get_thread_id omp_get_thread_num
-
-static void __init_const_globals(void* __stream, int8_t* __restrict__ __module_data, uint8_t* __restrict__ buffer_14, uint8_t* __restrict__ buffer_15, uint8_t* __restrict__ buffer_16, uint8_t* __restrict__ buffer_36) noexcept __attribute__((nonnull (2,3,4,5,6)));
-extern "C" void* sc_aligned_malloc(void* stream, uint64_t size) noexcept __attribute__((returns_nonnull))  __attribute__((malloc));
-static bool outerloop_4096_partition_cast_reduce_mul_reorder_39(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ reorder_49_outs_0, uint8_t* __restrict__ cast_38_ins_0, int32_t* __restrict__ mul_41_ins_1) noexcept __attribute__((nonnull (2,3,4,5)));
-static void outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder(void* __stream, int8_t* __restrict__ __module_data, uint8_t* __restrict__ buffer_11, uint8_t* __restrict__ buffer_10, int8_t* __restrict__ buffer_9, int32_t* __restrict__ buffer_8, int32_t* __restrict__ buffer_7, float* __restrict__ buffer_6, float* __restrict__ buffer_5, int32_t* __restrict__ buffer_4, int8_t* __restrict__ buffer_3, int32_t* __restrict__ buffer_2, float* __restrict__ buffer_1, float* __restrict__ buffer_0) noexcept __attribute__((nonnull (2,3,4,5,6,7,8,9,10,11,12,13,14)));
-extern "C" void sc_aligned_free(void* stream, void* ptr) noexcept;
-extern "C" void sc_parallel_call_managed(void* func, uint64_t flags, void* stream, int8_t* env, uint64_t begin, uint64_t end, uint64_t step, generic_val* args) noexcept;
-static void cast__80_closure_0_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static void sub__100_closure_1_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static void cast__110_closure_2_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static void cast__250_closure_3_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static void reduce__260_closure_4_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static void cast__40_closure_5_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static void sub__60_closure_6_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static void cast__70_closure_7_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static void cast__190_closure_8_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static void reduce__200_closure_9_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static void reorder__310_closure_10_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static void reorder__300_closure_11_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static void outerloop_4096_partition_cast_reduce_mul_reorder_390_closure_12_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static bool outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder_12(void* __stream, int8_t* __restrict__ __module_data, uint8_t* __restrict__ reorder_31_outs_0, uint8_t* __restrict__ quantized_managed_matmul_core_2_ins_0, int8_t* __restrict__ quantized_managed_matmul_core_2_ins_1, int32_t* __restrict__ sub_4_ins_1, int32_t* __restrict__ sub_6_ins_1, float* __restrict__ mul_9_ins_1, float* __restrict__ add_12_ins_1, int8_t* __restrict__ quantized_managed_matmul_core_19_ins_1, int32_t* __restrict__ mul_17_ins_1, int32_t* __restrict__ sub_22_ins_1, float* __restrict__ mul_25_ins_1, float* __restrict__ add_28_ins_1) noexcept __attribute__((nonnull (2,3,4,5,6,7,8,9,10,11,12,13,14)));
-static void outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder_120_closure_13_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
-static bool cast__8(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (2,3,4)));
-static bool sub__10(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1) noexcept __attribute__((nonnull (2,3,4,5)));
-static bool cast__11(void* __stream, int8_t* __restrict__ __module_data, int8_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (2,3,4)));
-static bool cast__25(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (2,3,4)));
-static bool reduce__26(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (2,3,4)));
-static bool mul__28(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1) noexcept __attribute__((nonnull (2,3,4,5)));
-static bool cast__4(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (2,3,4)));
-static bool sub__6(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1) noexcept __attribute__((nonnull (2,3,4,5)));
-static bool cast__7(void* __stream, int8_t* __restrict__ __module_data, int8_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (2,3,4)));
-static bool cast__19(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (2,3,4)));
-static bool reduce__20(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (2,3,4)));
-static bool mul__22(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1) noexcept __attribute__((nonnull (2,3,4,5)));
-static bool sub__37(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1) noexcept __attribute__((nonnull (2,3,4,5)));
-static bool sub__36(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1) noexcept __attribute__((nonnull (2,3,4,5)));
-static bool mul__34(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (2,3,4,5)));
-static bool mul__35(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (2,3,4,5)));
-static bool reorder__31(void* __stream, int8_t* __restrict__ __module_data, int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (2,3,4)));
-static bool reorder__30(void* __stream, int8_t* __restrict__ __module_data, int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (2,3,4)));
-extern "C" void* dnnl_brgemm_func(int32_t M, int32_t N, int32_t K, int32_t LDA, int32_t LDB, int32_t LDC, int32_t stride_a, int32_t stride_b, float beta, int32_t dtypeA, int32_t dtypeB, void* brg_attrs, void* bd_mask, void* postops_setting) noexcept;
-static void mlp(void* __stream, int8_t* __restrict__ __module_data, uint8_t* __restrict__ buffer_14, uint8_t* __restrict__ buffer_15, uint8_t* __restrict__ buffer_16, uint8_t* __restrict__ buffer_36) noexcept __attribute__((nonnull (2,3,4,5,6)));
-extern "C" void __sc_init__(void* __stream, int8_t* __restrict__ __module_data) noexcept __attribute__((nonnull (2)));
-static void cast__80_closure_0(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, uint8_t* __restrict__ __ins_0, int32_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,4,5)));
-static void sub__100_closure_1(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1, int32_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,4,5,6)));
-static void cast__110_closure_2(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, int32_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,4,5)));
-static void cast__250_closure_3(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, int8_t* __restrict__ __ins_0, int32_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,4,5)));
-static void reduce__260_closure_4(void* __stream, int8_t* __restrict__ __module_data, uint64_t _fuseiter_263, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,4,5)));
-static void cast__40_closure_5(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, uint8_t* __restrict__ __ins_0, int32_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,4,5)));
-static void sub__60_closure_6(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1, int32_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,4,5,6)));
-static void cast__70_closure_7(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, int32_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,4,5)));
-static void cast__190_closure_8(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, int8_t* __restrict__ __ins_0, int32_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,4,5)));
-static void reduce__200_closure_9(void* __stream, int8_t* __restrict__ __module_data, uint64_t _fuseiter_280, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,4,5)));
-static void reorder__310_closure_10(void* __stream, int8_t* __restrict__ __module_data, uint64_t fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,4,5)));
-static void reorder__300_closure_11(void* __stream, int8_t* __restrict__ __module_data, uint64_t fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,4,5)));
-extern "C" void* sc_thread_aligned_malloc(void* stream, uint64_t size) noexcept __attribute__((returns_nonnull))  __attribute__((malloc));
-extern "C" void sc_thread_aligned_free(void* stream, void* ptr) noexcept;
-static void outerloop_4096_partition_cast_reduce_mul_reorder_390_closure_12(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, uint8_t* __restrict__ cast_38_ins_0, int32_t* __restrict__ mul_41_ins_1, int32_t* __restrict__ reorder_49_outs_0) noexcept __attribute__((nonnull (2,4,5,6)));
-extern "C" int32_t sc_get_thread_id() noexcept __attribute__((const));
-extern "C" void dnnl_brgemm_call(void* func, void* A, void* B, void* C, int32_t num, void* stream) noexcept;
-static void outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder_120_closure_13(void* __stream, int8_t* __restrict__ __module_data, uint64_t m_s_0, uint8_t* __restrict__ quantized_managed_matmul_core_2_ins_0, int8_t* __restrict__ quantized_managed_matmul_core_2_ins_1, int32_t* __restrict__ sub_4_ins_1, int32_t* __restrict__ sub_6_ins_1, float* __restrict__ mul_9_ins_1, float* __restrict__ add_12_ins_1, int32_t* __restrict__ mul_17_ins_1, int8_t* __restrict__ quantized_managed_matmul_core_19_ins_1, int32_t* __restrict__ sub_22_ins_1, float* __restrict__ mul_25_ins_1, float* __restrict__ add_28_ins_1, uint8_t* __restrict__ reorder_31_outs_0) noexcept __attribute__((nonnull (2,4,5,6,7,8,9,10,11,12,13,14,15)));
-
-
-static void mlp(void* __stream, int8_t* __restrict__ __module_data, uint8_t* __restrict__ buffer_14, uint8_t* __restrict__ buffer_15, uint8_t* __restrict__ buffer_16, uint8_t* __restrict__ buffer_36) noexcept{
-  bool& is_init = *(bool*)(__module_data + 0);
-  int32_t* folded_const_2 = (int32_t*)&__module_data[32UL];
-  int8_t* folded_const_19 = (int8_t*)&__module_data[1056960UL];
-  int32_t* folded_const_15 = (int32_t*)&__module_data[4224UL];
-  float* folded_const_16 = (float*)&__module_data[8320UL];
-  float* folded_const_9 = (float*)&__module_data[60UL];
-  int32_t* folded_const_5 = (int32_t*)&__module_data[44UL];
-  int8_t* folded_const_18 = (int8_t*)&__module_data[8384UL];
-  int32_t* folded_const_14 = (int32_t*)&__module_data[128UL];
-  float* folded_const_17 = (float*)&__module_data[8324UL];
-  float* folded_const_6 = (float*)&__module_data[48UL];
-  if (!is_init) {
-    __init_const_globals(__stream, __module_data, buffer_14, buffer_15, buffer_16, buffer_36);
-  }
-  // [s32 [128, 1, 32, 1] @ AB32a1b]
-  int32_t* buffer_35 = (int32_t*)sc_aligned_malloc(__stream, 16384UL);
-  outerloop_4096_partition_cast_reduce_mul_reorder_39(__stream, __module_data, buffer_35, buffer_16, folded_const_2);
-  outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder(__stream, __module_data, buffer_36, buffer_16, folded_const_19, buffer_35, folded_const_15, folded_const_16, folded_const_9, folded_const_5, folded_const_18, folded_const_14, folded_const_17, folded_const_6);
-  sc_aligned_free(__stream, buffer_35);
-}
-
-static bool cast__8(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0) noexcept{
-  generic_val __tempargs0[2UL];
-  __tempargs0[0UL] = __ins_0;
-  __tempargs0[1UL] = __outs_0;
-  sc_parallel_call_managed((void*)&cast__80_closure_0_0wrapper, 0UL, __stream, __module_data, 0UL, 256UL, 1UL, __tempargs0);
-  return true;
-}
-
-static bool sub__10(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1) noexcept{
-  generic_val __tempargs1[3UL];
-  __tempargs1[0UL] = __ins_0;
-  __tempargs1[1UL] = __ins_1;
-  __tempargs1[2UL] = __outs_0;
-  sc_parallel_call_managed((void*)&sub__100_closure_1_0wrapper, 0UL, __stream, __module_data, 0UL, 512UL, 1UL, __tempargs1);
-  return true;
-}
-
-static bool cast__11(void* __stream, int8_t* __restrict__ __module_data, int8_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0) noexcept{
-  generic_val __tempargs2[2UL];
-  __tempargs2[0UL] = __ins_0;
-  __tempargs2[1UL] = __outs_0;
-  sc_parallel_call_managed((void*)&cast__110_closure_2_0wrapper, 0UL, __stream, __module_data, 0UL, 256UL, 1UL, __tempargs2);
-  return true;
-}
-
-static bool cast__25(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
-  generic_val __tempargs3[2UL];
-  __tempargs3[0UL] = __ins_0;
-  __tempargs3[1UL] = __outs_0;
-  sc_parallel_call_managed((void*)&cast__250_closure_3_0wrapper, 0UL, __stream, __module_data, 0UL, 256UL, 1UL, __tempargs3);
-  return true;
-}
-
-static bool reduce__26(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0) noexcept{
-  generic_val __tempargs4[2UL];
-  __tempargs4[0UL] = __ins_0;
-  __tempargs4[1UL] = __outs_0;
-  sc_parallel_call_managed((void*)&reduce__260_closure_4_0wrapper, 0UL, __stream, __module_data, 0UL, 1024UL, 16UL, __tempargs4);
-  return true;
-}
-
-static bool mul__28(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1) noexcept{
-  for (uint64_t _fuseiter_265 = 0UL; _fuseiter_265 < 1024UL; _fuseiter_265 += 16UL) {
-    vec_s32x16 __cached_0;
-    __cached_0 = vec_s32x16::load(&__ins_0[_fuseiter_265]);
-    vec_s32x16 __cached_1;
-    __cached_1 = (__cached_0 * vec_s32x16(__ins_1[0]));
-    vec_s32x16::store(__cached_1, &__outs_0[_fuseiter_265]);
-  }
-  return true;
-}
-
-static bool cast__4(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0) noexcept{
-  generic_val __tempargs5[2UL];
-  __tempargs5[0UL] = __ins_0;
-  __tempargs5[1UL] = __outs_0;
-  sc_parallel_call_managed((void*)&cast__40_closure_5_0wrapper, 0UL, __stream, __module_data, 0UL, 256UL, 1UL, __tempargs5);
-  return true;
-}
-
-static bool sub__6(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1) noexcept{
-  generic_val __tempargs6[3UL];
-  __tempargs6[0UL] = __ins_0;
-  __tempargs6[1UL] = __ins_1;
-  __tempargs6[2UL] = __outs_0;
-  sc_parallel_call_managed((void*)&sub__60_closure_6_0wrapper, 0UL, __stream, __module_data, 0UL, 512UL, 1UL, __tempargs6);
-  return true;
-}
-
-static bool cast__7(void* __stream, int8_t* __restrict__ __module_data, int8_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0) noexcept{
-  generic_val __tempargs7[2UL];
-  __tempargs7[0UL] = __ins_0;
-  __tempargs7[1UL] = __outs_0;
-  sc_parallel_call_managed((void*)&cast__70_closure_7_0wrapper, 0UL, __stream, __module_data, 0UL, 256UL, 1UL, __tempargs7);
-  return true;
-}
-
-static bool cast__19(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
-  generic_val __tempargs8[2UL];
-  __tempargs8[0UL] = __ins_0;
-  __tempargs8[1UL] = __outs_0;
-  sc_parallel_call_managed((void*)&cast__190_closure_8_0wrapper, 0UL, __stream, __module_data, 0UL, 256UL, 1UL, __tempargs8);
-  return true;
-}
-
-static bool reduce__20(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0) noexcept{
-  generic_val __tempargs9[2UL];
-  __tempargs9[0UL] = __ins_0;
-  __tempargs9[1UL] = __outs_0;
-  sc_parallel_call_managed((void*)&reduce__200_closure_9_0wrapper, 0UL, __stream, __module_data, 0UL, 1024UL, 16UL, __tempargs9);
-  return true;
-}
-
-static bool mul__22(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1) noexcept{
-  for (uint64_t _fuseiter_282 = 0UL; _fuseiter_282 < 1024UL; _fuseiter_282 += 16UL) {
-    vec_s32x16 __cached_0;
-    __cached_0 = vec_s32x16::load(&__ins_0[_fuseiter_282]);
-    vec_s32x16 __cached_1;
-    __cached_1 = (__cached_0 * vec_s32x16(__ins_1[0]));
-    vec_s32x16::store(__cached_1, &__outs_0[_fuseiter_282]);
-  }
-  return true;
-}
-
-static bool sub__37(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1) noexcept{
-  for (uint64_t fused_0fused_0__itr_0____itr_1_0____itr_2_1_0inner = 0UL; fused_0fused_0__itr_0____itr_1_0____itr_2_1_0inner < 16UL; fused_0fused_0__itr_0____itr_1_0____itr_2_1_0inner += 1UL) {
-    for (uint64_t _fuseiter_287 = 0UL; _fuseiter_287 < 64UL; _fuseiter_287 += 16UL) {
-      vec_s32x16 __cached_0;
-      __cached_0 = vec_s32x16::load(&__ins_0[((fused_0fused_0__itr_0____itr_1_0____itr_2_1_0inner * 64UL) + _fuseiter_287)]);
-      vec_s32x16 __cached_1;
-      __cached_1 = (__cached_0 - vec_s32x16(__ins_1[0]));
-      vec_s32x16::store(__cached_1, &__outs_0[((fused_0fused_0__itr_0____itr_1_0____itr_2_1_0inner * 64UL) + _fuseiter_287)]);
-    }
-  }
-  return true;
-}
-
-static bool sub__36(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ __outs_0, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1) noexcept{
-  for (uint64_t fused_0fused_0__itr_0____itr_1_2____itr_2_3_0inner = 0UL; fused_0fused_0__itr_0____itr_1_2____itr_2_3_0inner < 16UL; fused_0fused_0__itr_0____itr_1_2____itr_2_3_0inner += 1UL) {
-    for (uint64_t _fuseiter_292 = 0UL; _fuseiter_292 < 64UL; _fuseiter_292 += 16UL) {
-      vec_s32x16 __cached_0;
-      __cached_0 = vec_s32x16::load(&__ins_0[((fused_0fused_0__itr_0____itr_1_2____itr_2_3_0inner * 64UL) + _fuseiter_292)]);
-      vec_s32x16 __cached_1;
-      __cached_1 = (__cached_0 - vec_s32x16(__ins_1[0]));
-      vec_s32x16::store(__cached_1, &__outs_0[((fused_0fused_0__itr_0____itr_1_2____itr_2_3_0inner * 64UL) + _fuseiter_292)]);
-    }
-  }
-  return true;
-}
-
-static bool mul__34(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
-  float __cached_0;
-  __cached_0 = __ins_0[0UL];
-  float __cached_1;
-  __cached_1 = __ins_1[0UL];
-  float __cached_2;
-  __cached_2 = (__cached_0 * __cached_1);
-  __outs_0[0UL] = __cached_2;
-  return true;
-}
-
-static bool mul__35(void* __stream, int8_t* __restrict__ __module_data, float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
-  float __cached_0;
-  __cached_0 = __ins_0[0UL];
-  float __cached_1;
-  __cached_1 = __ins_1[0UL];
-  float __cached_2;
-  __cached_2 = (__cached_0 * __cached_1);
-  __outs_0[0UL] = __cached_2;
-  return true;
-}
-
-static bool reorder__31(void* __stream, int8_t* __restrict__ __module_data, int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
-  generic_val __tempargs10[2UL];
-  __tempargs10[0UL] = __ins_0;
-  __tempargs10[1UL] = __outs_0;
-  sc_parallel_call_managed((void*)&reorder__310_closure_10_0wrapper, 0UL, __stream, __module_data, 0UL, 1024UL, 1UL, __tempargs10);
-  return true;
-}
-
-static bool reorder__30(void* __stream, int8_t* __restrict__ __module_data, int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
-  generic_val __tempargs11[2UL];
-  __tempargs11[0UL] = __ins_0;
-  __tempargs11[1UL] = __outs_0;
-  sc_parallel_call_managed((void*)&reorder__300_closure_11_0wrapper, 0UL, __stream, __module_data, 0UL, 1024UL, 1UL, __tempargs11);
-  return true;
-}
-
-static bool outerloop_4096_partition_cast_reduce_mul_reorder_39(void* __stream, int8_t* __restrict__ __module_data, int32_t* __restrict__ reorder_49_outs_0, uint8_t* __restrict__ cast_38_ins_0, int32_t* __restrict__ mul_41_ins_1) noexcept{
-  generic_val __tempargs12[3UL];
-  __tempargs12[0UL] = cast_38_ins_0;
-  __tempargs12[1UL] = mul_41_ins_1;
-  __tempargs12[2UL] = reorder_49_outs_0;
-  sc_parallel_call_managed((void*)&outerloop_4096_partition_cast_reduce_mul_reorder_390_closure_12_0wrapper, 0UL, __stream, __module_data, 0UL, 1024UL, 1UL, __tempargs12);
-  return true;
-}
-
-static void outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder(void* __stream, int8_t* __restrict__ __module_data, uint8_t* __restrict__ buffer_11, uint8_t* __restrict__ buffer_10, int8_t* __restrict__ buffer_9, int32_t* __restrict__ buffer_8, int32_t* __restrict__ buffer_7, float* __restrict__ buffer_6, float* __restrict__ buffer_5, int32_t* __restrict__ buffer_4, int8_t* __restrict__ buffer_3, int32_t* __restrict__ buffer_2, float* __restrict__ buffer_1, float* __restrict__ buffer_0) noexcept{
-  outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder_12(__stream, __module_data, buffer_11, buffer_10, buffer_9, buffer_8, buffer_7, buffer_6, buffer_5, buffer_3, buffer_4, buffer_2, buffer_1, buffer_0);
-}
-
-static bool outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder_12(void* __stream, int8_t* __restrict__ __module_data, uint8_t* __restrict__ reorder_31_outs_0, uint8_t* __restrict__ quantized_managed_matmul_core_2_ins_0, int8_t* __restrict__ quantized_managed_matmul_core_2_ins_1, int32_t* __restrict__ sub_4_ins_1, int32_t* __restrict__ sub_6_ins_1, float* __restrict__ mul_9_ins_1, float* __restrict__ add_12_ins_1, int8_t* __restrict__ quantized_managed_matmul_core_19_ins_1, int32_t* __restrict__ mul_17_ins_1, int32_t* __restrict__ sub_22_ins_1, float* __restrict__ mul_25_ins_1, float* __restrict__ add_28_ins_1) noexcept{
-  alignas(64) generic_val __tempargs13[12UL];
-  __tempargs13[0UL] = quantized_managed_matmul_core_2_ins_0;
-  __tempargs13[1UL] = quantized_managed_matmul_core_2_ins_1;
-  __tempargs13[2UL] = sub_4_ins_1;
-  __tempargs13[3UL] = sub_6_ins_1;
-  __tempargs13[4UL] = mul_9_ins_1;
-  __tempargs13[5UL] = add_12_ins_1;
-  __tempargs13[6UL] = mul_17_ins_1;
-  __tempargs13[7UL] = quantized_managed_matmul_core_19_ins_1;
-  __tempargs13[8UL] = sub_22_ins_1;
-  __tempargs13[9UL] = mul_25_ins_1;
-  __tempargs13[10UL] = add_28_ins_1;
-  __tempargs13[11UL] = reorder_31_outs_0;
-  sc_parallel_call_managed((void*)&outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder_120_closure_13_0wrapper, 0UL, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs13);
-  return true;
-}
-
-static void __init_const_globals(void* __stream, int8_t* __restrict__ __module_data, uint8_t* __restrict__ buffer_14, uint8_t* __restrict__ buffer_15, uint8_t* __restrict__ buffer_16, uint8_t* __restrict__ buffer_36) noexcept{
-  int32_t* folded_const_12 = (int32_t*)&__module_data[72UL];
-  int32_t* folded_const_1 = (int32_t*)&__module_data[28UL];
-  int32_t* folded_const_13 = (int32_t*)&__module_data[76UL];
-  int32_t* folded_const_4 = (int32_t*)&__module_data[40UL];
-  int32_t* folded_const_14 = (int32_t*)&__module_data[128UL];
-  int32_t* folded_const_3 = (int32_t*)&__module_data[36UL];
-  int32_t* folded_const_15 = (int32_t*)&__module_data[4224UL];
-  int32_t* folded_const_0 = (int32_t*)&__module_data[24UL];
-  float* folded_const_16 = (float*)&__module_data[8320UL];
-  float* folded_const_11 = (float*)&__module_data[68UL];
-  float* folded_const_10 = (float*)&__module_data[64UL];
-  float* folded_const_17 = (float*)&__module_data[8324UL];
-  float* folded_const_8 = (float*)&__module_data[56UL];
-  float* folded_const_7 = (float*)&__module_data[52UL];
-  int8_t* folded_const_18 = (int8_t*)&__module_data[8384UL];
-  int8_t* folded_const_19 = (int8_t*)&__module_data[1056960UL];
-  bool& is_init = *(bool*)(__module_data + 0);
-  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 9441280UL);
-  // [s32 [1024, 1024] @ AB]
-  int32_t* buffer_17 = (int32_t*)&__rescheduled_0[0UL];
-  cast__8(__stream, __module_data, buffer_17, buffer_15);
-  // [s32 [1024, 1024] @ AB]
-  int32_t* buffer_18 = (int32_t*)&__rescheduled_0[4194304UL];
-  sub__10(__stream, __module_data, buffer_18, buffer_17, folded_const_12);
-  // [s8 [1024, 1024] @ AB]
-  int8_t* buffer_19 = (int8_t*)&__rescheduled_0[0UL];
-  cast__11(__stream, __module_data, buffer_19, buffer_18);
-  // [s32 [1024, 1024] @ AB]
-  int32_t* buffer_20 = (int32_t*)&__rescheduled_0[1048576UL];
-  cast__25(__stream, __module_data, buffer_20, buffer_19);
-  // [s32 [1, 1024] @ AB]
-  int32_t* buffer_21 = (int32_t*)&__rescheduled_0[5242880UL];
-  reduce__26(__stream, __module_data, buffer_21, buffer_20);
-  // [s32 [1, 1024] @ AB]
-  int32_t* buffer_22 = (int32_t*)&__rescheduled_0[1048576UL];
-  mul__28(__stream, __module_data, buffer_22, buffer_21, folded_const_1);
-  // [s32 [1024, 1024] @ AB]
-  int32_t* buffer_23 = (int32_t*)&__rescheduled_0[1052672UL];
-  cast__4(__stream, __module_data, buffer_23, buffer_14);
-  // [s32 [1024, 1024] @ AB]
-  int32_t* buffer_24 = (int32_t*)&__rescheduled_0[5246976UL];
-  sub__6(__stream, __module_data, buffer_24, buffer_23, folded_const_13);
-  // [s8 [1024, 1024] @ AB]
-  int8_t* buffer_25 = (int8_t*)&__rescheduled_0[1052672UL];
-  cast__7(__stream, __module_data, buffer_25, buffer_24);
-  // [s32 [1024, 1024] @ AB]
-  int32_t* buffer_26 = (int32_t*)&__rescheduled_0[2101248UL];
-  cast__19(__stream, __module_data, buffer_26, buffer_25);
-  // [s32 [1, 1024] @ AB]
-  int32_t* buffer_27 = (int32_t*)&__rescheduled_0[6295552UL];
-  reduce__20(__stream, __module_data, buffer_27, buffer_26);
-  // [s32 [1, 1024] @ AB]
-  int32_t* buffer_28 = (int32_t*)&__rescheduled_0[2101248UL];
-  mul__22(__stream, __module_data, buffer_28, buffer_27, folded_const_4);
-  sub__37(__stream, __module_data, folded_const_14, &buffer_28[0UL], folded_const_3);
-  sub__36(__stream, __module_data, folded_const_15, &buffer_22[0UL], folded_const_0);
-  mul__34(__stream, __module_data, folded_const_16, folded_const_11, folded_const_10);
-  mul__35(__stream, __module_data, folded_const_17, folded_const_8, folded_const_7);
-  reorder__31(__stream, __module_data, folded_const_18, buffer_25);
-  reorder__30(__stream, __module_data, folded_const_19, buffer_19);
-  is_init = true;
-  sc_aligned_free(__stream, __rescheduled_0);
-}
-
-extern "C" void sc_init_mlp(void* __stream, int8_t* __restrict__ __module_data) noexcept{
-  bool& is_init = *(bool*)(__module_data + 0);
-  void*& __sc_kernel_cache = *(void**)(__module_data + 8);
-  void*& __sc_kernel_cache_1 = *(void**)(__module_data + 16);
-  is_init = false;
-  __sc_kernel_cache = dnnl_brgemm_func(32, 64, 64, 1024, 64, 64, 64, 4096, 0.f, 8, 7, ((void*)0), ((void*)0), ((void*)0));
-  __sc_kernel_cache_1 = dnnl_brgemm_func(32, 64, 64, 64, 64, 64, 2048, 4096, 0.f, 8, 7, ((void*)0), ((void*)0), ((void*)0));
-}
-
-extern "C" void mlp_0wrapper_impl(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  generic_val __cached_2;
-  __cached_2 = args[2UL];
-  generic_val __cached_3;
-  __cached_3 = args[3UL];
-  mlp(__stream, __module_data, (uint8_t*)(__cached_0.v_ptr), (uint8_t*)(__cached_1.v_ptr), (uint8_t*)(__cached_2.v_ptr), (uint8_t*)(__cached_3.v_ptr));
-}
-
-static void cast__8_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  cast__8(__stream, __module_data, (int32_t*)(__cached_0.v_ptr), (uint8_t*)(__cached_1.v_ptr));
-}
-
-static void sub__10_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  generic_val __cached_2;
-  __cached_2 = args[2UL];
-  sub__10(__stream, __module_data, (int32_t*)(__cached_0.v_ptr), (int32_t*)(__cached_1.v_ptr), (int32_t*)(__cached_2.v_ptr));
-}
-
-static void cast__11_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  cast__11(__stream, __module_data, (int8_t*)(__cached_0.v_ptr), (int32_t*)(__cached_1.v_ptr));
-}
-
-static void cast__25_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  cast__25(__stream, __module_data, (int32_t*)(__cached_0.v_ptr), (int8_t*)(__cached_1.v_ptr));
-}
-
-static void reduce__26_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  reduce__26(__stream, __module_data, (int32_t*)(__cached_0.v_ptr), (int32_t*)(__cached_1.v_ptr));
-}
-
-static void mul__28_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  generic_val __cached_2;
-  __cached_2 = args[2UL];
-  mul__28(__stream, __module_data, (int32_t*)(__cached_0.v_ptr), (int32_t*)(__cached_1.v_ptr), (int32_t*)(__cached_2.v_ptr));
-}
-
-static void cast__4_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  cast__4(__stream, __module_data, (int32_t*)(__cached_0.v_ptr), (uint8_t*)(__cached_1.v_ptr));
-}
-
-static void sub__6_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  generic_val __cached_2;
-  __cached_2 = args[2UL];
-  sub__6(__stream, __module_data, (int32_t*)(__cached_0.v_ptr), (int32_t*)(__cached_1.v_ptr), (int32_t*)(__cached_2.v_ptr));
-}
-
-static void cast__7_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  cast__7(__stream, __module_data, (int8_t*)(__cached_0.v_ptr), (int32_t*)(__cached_1.v_ptr));
-}
-
-static void cast__19_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  cast__19(__stream, __module_data, (int32_t*)(__cached_0.v_ptr), (int8_t*)(__cached_1.v_ptr));
-}
-
-static void reduce__20_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  reduce__20(__stream, __module_data, (int32_t*)(__cached_0.v_ptr), (int32_t*)(__cached_1.v_ptr));
-}
-
-static void mul__22_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  generic_val __cached_2;
-  __cached_2 = args[2UL];
-  mul__22(__stream, __module_data, (int32_t*)(__cached_0.v_ptr), (int32_t*)(__cached_1.v_ptr), (int32_t*)(__cached_2.v_ptr));
-}
-
-static void sub__37_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  generic_val __cached_2;
-  __cached_2 = args[2UL];
-  sub__37(__stream, __module_data, (int32_t*)(__cached_0.v_ptr), (int32_t*)(__cached_1.v_ptr), (int32_t*)(__cached_2.v_ptr));
-}
-
-static void sub__36_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  generic_val __cached_2;
-  __cached_2 = args[2UL];
-  sub__36(__stream, __module_data, (int32_t*)(__cached_0.v_ptr), (int32_t*)(__cached_1.v_ptr), (int32_t*)(__cached_2.v_ptr));
-}
-
-static void mul__34_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  generic_val __cached_2;
-  __cached_2 = args[2UL];
-  mul__34(__stream, __module_data, (float*)(__cached_0.v_ptr), (float*)(__cached_1.v_ptr), (float*)(__cached_2.v_ptr));
-}
-
-static void mul__35_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  generic_val __cached_2;
-  __cached_2 = args[2UL];
-  mul__35(__stream, __module_data, (float*)(__cached_0.v_ptr), (float*)(__cached_1.v_ptr), (float*)(__cached_2.v_ptr));
-}
-
-static void reorder__31_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  reorder__31(__stream, __module_data, (int8_t*)(__cached_0.v_ptr), (int8_t*)(__cached_1.v_ptr));
-}
-
-static void reorder__30_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  reorder__30(__stream, __module_data, (int8_t*)(__cached_0.v_ptr), (int8_t*)(__cached_1.v_ptr));
-}
-
-static void outerloop_4096_partition_cast_reduce_mul_reorder_39_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  generic_val __cached_2;
-  __cached_2 = args[2UL];
-  outerloop_4096_partition_cast_reduce_mul_reorder_39(__stream, __module_data, (int32_t*)(__cached_0.v_ptr), (uint8_t*)(__cached_1.v_ptr), (int32_t*)(__cached_2.v_ptr));
-}
-
-static void outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  generic_val __cached_2;
-  __cached_2 = args[2UL];
-  generic_val __cached_3;
-  __cached_3 = args[3UL];
-  generic_val __cached_4;
-  __cached_4 = args[4UL];
-  generic_val __cached_5;
-  __cached_5 = args[5UL];
-  generic_val __cached_6;
-  __cached_6 = args[6UL];
-  generic_val __cached_7;
-  __cached_7 = args[7UL];
-  generic_val __cached_8;
-  __cached_8 = args[8UL];
-  generic_val __cached_9;
-  __cached_9 = args[9UL];
-  generic_val __cached_10;
-  __cached_10 = args[10UL];
-  generic_val __cached_11;
-  __cached_11 = args[11UL];
-  outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder(__stream, __module_data, (uint8_t*)(__cached_0.v_ptr), (uint8_t*)(__cached_1.v_ptr), (int8_t*)(__cached_2.v_ptr), (int32_t*)(__cached_3.v_ptr), (int32_t*)(__cached_4.v_ptr), (float*)(__cached_5.v_ptr), (float*)(__cached_6.v_ptr), (int32_t*)(__cached_7.v_ptr), (int8_t*)(__cached_8.v_ptr), (int32_t*)(__cached_9.v_ptr), (float*)(__cached_10.v_ptr), (float*)(__cached_11.v_ptr));
-}
-
-static void outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder_12_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  generic_val __cached_0;
-  __cached_0 = args[0UL];
-  generic_val __cached_1;
-  __cached_1 = args[1UL];
-  generic_val __cached_2;
-  __cached_2 = args[2UL];
-  generic_val __cached_3;
-  __cached_3 = args[3UL];
-  generic_val __cached_4;
-  __cached_4 = args[4UL];
-  generic_val __cached_5;
-  __cached_5 = args[5UL];
-  generic_val __cached_6;
-  __cached_6 = args[6UL];
-  generic_val __cached_7;
-  __cached_7 = args[7UL];
-  generic_val __cached_8;
-  __cached_8 = args[8UL];
-  generic_val __cached_9;
-  __cached_9 = args[9UL];
-  generic_val __cached_10;
-  __cached_10 = args[10UL];
-  generic_val __cached_11;
-  __cached_11 = args[11UL];
-  outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder_12(__stream, __module_data, (uint8_t*)(__cached_0.v_ptr), (uint8_t*)(__cached_1.v_ptr), (int8_t*)(__cached_2.v_ptr), (int32_t*)(__cached_3.v_ptr), (int32_t*)(__cached_4.v_ptr), (float*)(__cached_5.v_ptr), (float*)(__cached_6.v_ptr), (int8_t*)(__cached_7.v_ptr), (int32_t*)(__cached_8.v_ptr), (int32_t*)(__cached_9.v_ptr), (float*)(__cached_10.v_ptr), (float*)(__cached_11.v_ptr));
-}
-
-static void cast__80_closure_0(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, uint8_t* __restrict__ __ins_0, int32_t* __restrict__ __outs_0) noexcept{
-  for (uint64_t __itr_0_0inner = 0UL; __itr_0_0inner < 4UL; __itr_0_0inner += 1UL) {
-    for (uint64_t _fuseiter251 = 0UL; _fuseiter251 < 1024UL; _fuseiter251 += 16UL) {
-      vec_u8x16 __cached_0;
-      __cached_0 = vec_u8x16::load(&__ins_0[((((__itr_0_0outer * 4UL) + __itr_0_0inner) * 1024UL) + _fuseiter251)]);
-      vec_s32x16 __cached_1;
-      __cached_1 = (vec_s32x16)(__cached_0);
-      vec_s32x16::store(__cached_1, &__outs_0[((((__itr_0_0outer * 4UL) + __itr_0_0inner) * 1024UL) + _fuseiter251)]);
-    }
-  }
-}
-
-static void cast__80_closure_0_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  cast__80_closure_0(__stream, __module_data, i, (uint8_t*)(args[0UL].v_ptr), (int32_t*)(args[1UL].v_ptr));
-}
-
-static void sub__100_closure_1(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1, int32_t* __restrict__ __outs_0) noexcept{
-  for (uint64_t __itr_0_0inner = 0UL; __itr_0_0inner < 2UL; __itr_0_0inner += 1UL) {
-    for (uint64_t _fuseiter_254 = 0UL; _fuseiter_254 < 1024UL; _fuseiter_254 += 16UL) {
-      vec_s32x16 __cached_0;
-      __cached_0 = vec_s32x16::load(&__ins_0[((((__itr_0_0outer * 2UL) + __itr_0_0inner) * 1024UL) + _fuseiter_254)]);
-      vec_s32x16 __cached_1;
-      __cached_1 = (__cached_0 - vec_s32x16(__ins_1[0]));
-      vec_s32x16::store(__cached_1, &__outs_0[((((__itr_0_0outer * 2UL) + __itr_0_0inner) * 1024UL) + _fuseiter_254)]);
-    }
-  }
-}
-
-static void sub__100_closure_1_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  sub__100_closure_1(__stream, __module_data, i, (int32_t*)(args[0UL].v_ptr), (int32_t*)(args[1UL].v_ptr), (int32_t*)(args[2UL].v_ptr));
-}
-
-static void cast__110_closure_2(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, int32_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
-  for (uint64_t __itr_0_0inner = 0UL; __itr_0_0inner < 4UL; __itr_0_0inner += 1UL) {
-    for (uint64_t _fuseiter257 = 0UL; _fuseiter257 < 1024UL; _fuseiter257 += 16UL) {
-      vec_s32x16 __cached_0;
-      __cached_0 = vec_s32x16::load(&__ins_0[((((__itr_0_0outer * 4UL) + __itr_0_0inner) * 1024UL) + _fuseiter257)]);
-      vec_s8x16 __cached_1;
-      __cached_1 = (vec_s8x16)(__cached_0);
-      vec_s8x16::store(__cached_1, &__outs_0[((((__itr_0_0outer * 4UL) + __itr_0_0inner) * 1024UL) + _fuseiter257)]);
-    }
-  }
-}
-
-static void cast__110_closure_2_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  cast__110_closure_2(__stream, __module_data, i, (int32_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
-}
-
-static void cast__250_closure_3(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, int8_t* __restrict__ __ins_0, int32_t* __restrict__ __outs_0) noexcept{
-  for (uint64_t __itr_0_0inner = 0UL; __itr_0_0inner < 4UL; __itr_0_0inner += 1UL) {
-    for (uint64_t _fuseiter260 = 0UL; _fuseiter260 < 1024UL; _fuseiter260 += 16UL) {
-      vec_s8x16 __cached_0;
-      __cached_0 = vec_s8x16::load(&__ins_0[((((__itr_0_0outer * 4UL) + __itr_0_0inner) * 1024UL) + _fuseiter260)]);
-      vec_s32x16 __cached_1;
-      __cached_1 = (vec_s32x16)(__cached_0);
-      vec_s32x16::store(__cached_1, &__outs_0[((((__itr_0_0outer * 4UL) + __itr_0_0inner) * 1024UL) + _fuseiter260)]);
-    }
-  }
-}
-
-static void cast__250_closure_3_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  cast__250_closure_3(__stream, __module_data, i, (int8_t*)(args[0UL].v_ptr), (int32_t*)(args[1UL].v_ptr));
-}
-
-static void reduce__260_closure_4(void* __stream, int8_t* __restrict__ __module_data, uint64_t _fuseiter_263, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __outs_0) noexcept{
-  vec_s32x16 reduce__12;
-  reduce__12 = vec_s32x16(0);
-  for (uint64_t _fuseiter_262 = 0UL; _fuseiter_262 < 1024UL; _fuseiter_262 += 1UL) {
-    vec_s32x16 __cached_0;
-    __cached_0 = vec_s32x16::load(&__ins_0[((_fuseiter_262 * 1024UL) + _fuseiter_263)]);
-    reduce__12 = (__cached_0 + reduce__12);
-  }
-  vec_s32x16 __cached_1;
-  __cached_1 = reduce__12;
-  vec_s32x16::store(__cached_1, &__outs_0[_fuseiter_263]);
-}
-
-static void reduce__260_closure_4_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  reduce__260_closure_4(__stream, __module_data, i, (int32_t*)(args[0UL].v_ptr), (int32_t*)(args[1UL].v_ptr));
-}
-
-static void cast__40_closure_5(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, uint8_t* __restrict__ __ins_0, int32_t* __restrict__ __outs_0) noexcept{
-  for (uint64_t __itr_0_0inner = 0UL; __itr_0_0inner < 4UL; __itr_0_0inner += 1UL) {
-    for (uint64_t _fuseiter268 = 0UL; _fuseiter268 < 1024UL; _fuseiter268 += 16UL) {
-      vec_u8x16 __cached_0;
-      __cached_0 = vec_u8x16::load(&__ins_0[((((__itr_0_0outer * 4UL) + __itr_0_0inner) * 1024UL) + _fuseiter268)]);
-      vec_s32x16 __cached_1;
-      __cached_1 = (vec_s32x16)(__cached_0);
-      vec_s32x16::store(__cached_1, &__outs_0[((((__itr_0_0outer * 4UL) + __itr_0_0inner) * 1024UL) + _fuseiter268)]);
-    }
-  }
-}
-
-static void cast__40_closure_5_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  cast__40_closure_5(__stream, __module_data, i, (uint8_t*)(args[0UL].v_ptr), (int32_t*)(args[1UL].v_ptr));
-}
-
-static void sub__60_closure_6(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __ins_1, int32_t* __restrict__ __outs_0) noexcept{
-  for (uint64_t __itr_0_0inner = 0UL; __itr_0_0inner < 2UL; __itr_0_0inner += 1UL) {
-    for (uint64_t _fuseiter_271 = 0UL; _fuseiter_271 < 1024UL; _fuseiter_271 += 16UL) {
-      vec_s32x16 __cached_0;
-      __cached_0 = vec_s32x16::load(&__ins_0[((((__itr_0_0outer * 2UL) + __itr_0_0inner) * 1024UL) + _fuseiter_271)]);
-      vec_s32x16 __cached_1;
-      __cached_1 = (__cached_0 - vec_s32x16(__ins_1[0]));
-      vec_s32x16::store(__cached_1, &__outs_0[((((__itr_0_0outer * 2UL) + __itr_0_0inner) * 1024UL) + _fuseiter_271)]);
-    }
-  }
-}
-
-static void sub__60_closure_6_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  sub__60_closure_6(__stream, __module_data, i, (int32_t*)(args[0UL].v_ptr), (int32_t*)(args[1UL].v_ptr), (int32_t*)(args[2UL].v_ptr));
-}
-
-static void cast__70_closure_7(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, int32_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
-  for (uint64_t __itr_0_0inner = 0UL; __itr_0_0inner < 4UL; __itr_0_0inner += 1UL) {
-    for (uint64_t _fuseiter274 = 0UL; _fuseiter274 < 1024UL; _fuseiter274 += 16UL) {
-      vec_s32x16 __cached_0;
-      __cached_0 = vec_s32x16::load(&__ins_0[((((__itr_0_0outer * 4UL) + __itr_0_0inner) * 1024UL) + _fuseiter274)]);
-      vec_s8x16 __cached_1;
-      __cached_1 = (vec_s8x16)(__cached_0);
-      vec_s8x16::store(__cached_1, &__outs_0[((((__itr_0_0outer * 4UL) + __itr_0_0inner) * 1024UL) + _fuseiter274)]);
-    }
-  }
-}
-
-static void cast__70_closure_7_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  cast__70_closure_7(__stream, __module_data, i, (int32_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
-}
-
-static void cast__190_closure_8(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, int8_t* __restrict__ __ins_0, int32_t* __restrict__ __outs_0) noexcept{
-  for (uint64_t __itr_0_0inner = 0UL; __itr_0_0inner < 4UL; __itr_0_0inner += 1UL) {
-    for (uint64_t _fuseiter277 = 0UL; _fuseiter277 < 1024UL; _fuseiter277 += 16UL) {
-      vec_s8x16 __cached_0;
-      __cached_0 = vec_s8x16::load(&__ins_0[((((__itr_0_0outer * 4UL) + __itr_0_0inner) * 1024UL) + _fuseiter277)]);
-      vec_s32x16 __cached_1;
-      __cached_1 = (vec_s32x16)(__cached_0);
-      vec_s32x16::store(__cached_1, &__outs_0[((((__itr_0_0outer * 4UL) + __itr_0_0inner) * 1024UL) + _fuseiter277)]);
-    }
-  }
-}
-
-static void cast__190_closure_8_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  cast__190_closure_8(__stream, __module_data, i, (int8_t*)(args[0UL].v_ptr), (int32_t*)(args[1UL].v_ptr));
-}
-
-static void reduce__200_closure_9(void* __stream, int8_t* __restrict__ __module_data, uint64_t _fuseiter_280, int32_t* __restrict__ __ins_0, int32_t* __restrict__ __outs_0) noexcept{
-  vec_s32x16 reduce__13;
-  reduce__13 = vec_s32x16(0);
-  for (uint64_t _fuseiter_279 = 0UL; _fuseiter_279 < 1024UL; _fuseiter_279 += 1UL) {
-    vec_s32x16 __cached_0;
-    __cached_0 = vec_s32x16::load(&__ins_0[((_fuseiter_279 * 1024UL) + _fuseiter_280)]);
-    reduce__13 = (__cached_0 + reduce__13);
-  }
-  vec_s32x16 __cached_1;
-  __cached_1 = reduce__13;
-  vec_s32x16::store(__cached_1, &__outs_0[_fuseiter_280]);
-}
-
-static void reduce__200_closure_9_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  reduce__200_closure_9(__stream, __module_data, i, (int32_t*)(args[0UL].v_ptr), (int32_t*)(args[1UL].v_ptr));
-}
-
-static void reorder__310_closure_10(void* __stream, int8_t* __restrict__ __module_data, uint64_t fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
-  for (uint64_t fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner = 0UL; fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner < 4UL; fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner += 1UL) {
-    for (uint64_t _fuseiter_301 = 0UL; _fuseiter_301 < 64UL; _fuseiter_301 += 16UL) {
-      vec_s8x16 row1_14;
-      vec_s8x16 row2_15;
-      vec_s8x16 row3_16;
-      vec_s8x16 row4_17;
-      vec_s8x16 __cached_0;
-      __cached_0 = vec_s8x16::load(&__ins_0[(((((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) % 16UL) * 4UL) + (((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 16UL) % 16UL) * 64UL)) * 1024UL) + (_fuseiter_301 + ((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 256UL) * 64UL)))]);
-      row1_14 = __cached_0;
-      vec_s8x16 __cached_1;
-      __cached_1 = vec_s8x16::load(&__ins_0[((((((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) % 16UL) * 4UL) + (((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 16UL) % 16UL) * 64UL)) + 1UL) * 1024UL) + (_fuseiter_301 + ((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 256UL) * 64UL)))]);
-      row2_15 = __cached_1;
-      vec_s8x16 __cached_2;
-      __cached_2 = vec_s8x16::load(&__ins_0[((((((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) % 16UL) * 4UL) + (((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 16UL) % 16UL) * 64UL)) + 2UL) * 1024UL) + (_fuseiter_301 + ((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 256UL) * 64UL)))]);
-      row3_16 = __cached_2;
-      vec_s8x16 __cached_3;
-      __cached_3 = vec_s8x16::load(&__ins_0[((((((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) % 16UL) * 4UL) + (((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 16UL) % 16UL) * 64UL)) + 3UL) * 1024UL) + (_fuseiter_301 + ((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 256UL) * 64UL)))]);
-      row4_17 = __cached_3;
-      vec_u8x16 xmm0;
-      vec_u8x16 xmm1;
-      vec_u8x16 xmm2;
-      vec_u8x16 xmm3;
-      vec_u8x16 xmm_tmp;
-      xmm0 = sc_reinterpret<vec_u8x16>(row1_14);
-      xmm1 = sc_reinterpret<vec_u8x16>(row2_15);
-      xmm2 = sc_reinterpret<vec_u8x16>(row3_16);
-      xmm3 = sc_reinterpret<vec_u8x16>(row4_17);
-      xmm_tmp = xmm0;
-      xmm0 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(285282304UL, 318968322UL, 352654340UL, 386340358UL)), xmm1);
-      xmm1 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(420026376UL, 453712394UL, 487398412UL, 521084430UL)), xmm1);
-      xmm_tmp = xmm2;
-      xmm2 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(285282304UL, 318968322UL, 352654340UL, 386340358UL)), xmm3);
-      xmm3 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(420026376UL, 453712394UL, 487398412UL, 521084430UL)), xmm3);
-      xmm_tmp = xmm0;
-      xmm0 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(286261504UL, 319947522UL, 353633540UL, 387319558UL)), xmm2);
-      xmm2 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(421005576UL, 454691594UL, 488377612UL, 522063630UL)), xmm2);
-      xmm_tmp = xmm1;
-      xmm1 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(286261504UL, 319947522UL, 353633540UL, 387319558UL)), xmm3);
-      xmm3 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(421005576UL, 454691594UL, 488377612UL, 522063630UL)), xmm3);
-      xmm_tmp = xmm1;
-      xmm1 = xmm2;
-      xmm2 = xmm_tmp;
-      row1_14 = sc_reinterpret<vec_s8x16>(xmm0);
-      row2_15 = sc_reinterpret<vec_s8x16>(xmm1);
-      row3_16 = sc_reinterpret<vec_s8x16>(xmm2);
-      row4_17 = sc_reinterpret<vec_s8x16>(xmm3);
-      vec_s8x16 __cached_4;
-      __cached_4 = row1_14;
-      vec_s8x16::store(__cached_4, &__outs_0[(((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 256UL) * 65536UL) + ((((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 16UL) % 16UL) * 4096UL) + (((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) % 16UL) * 256UL) + (_fuseiter_301 * 4UL))))]);
-      vec_s8x16 __cached_5;
-      __cached_5 = row2_15;
-      vec_s8x16::store(__cached_5, &__outs_0[(((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 256UL) * 65536UL) + ((((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 16UL) % 16UL) * 4096UL) + (((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) % 16UL) * 256UL) + ((_fuseiter_301 + 4UL) * 4UL))))]);
-      vec_s8x16 __cached_6;
-      __cached_6 = row3_16;
-      vec_s8x16::store(__cached_6, &__outs_0[(((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 256UL) * 65536UL) + ((((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 16UL) % 16UL) * 4096UL) + (((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) % 16UL) * 256UL) + ((_fuseiter_301 + 8UL) * 4UL))))]);
-      vec_s8x16 __cached_7;
-      __cached_7 = row4_17;
-      vec_s8x16::store(__cached_7, &__outs_0[(((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 256UL) * 65536UL) + ((((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) / 16UL) % 16UL) * 4096UL) + (((((fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0outer * 4UL) + fused_0fused_0_fuseiter_298___fuseiter_299_4___fuseiter_300_5_0inner) % 16UL) * 256UL) + ((_fuseiter_301 + 12UL) * 4UL))))]);
-    }
-  }
-}
-
-static void reorder__310_closure_10_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  reorder__310_closure_10(__stream, __module_data, i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
-}
-
-static void reorder__300_closure_11(void* __stream, int8_t* __restrict__ __module_data, uint64_t fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
-  for (uint64_t fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner = 0UL; fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner < 4UL; fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner += 1UL) {
-    for (uint64_t _fuseiter_306 = 0UL; _fuseiter_306 < 64UL; _fuseiter_306 += 16UL) {
-      vec_s8x16 row1_18;
-      vec_s8x16 row2_19;
-      vec_s8x16 row3_20;
-      vec_s8x16 row4_21;
-      vec_s8x16 __cached_0;
-      __cached_0 = vec_s8x16::load(&__ins_0[(((((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) % 16UL) * 4UL) + (((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 16UL) % 16UL) * 64UL)) * 1024UL) + (_fuseiter_306 + ((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 256UL) * 64UL)))]);
-      row1_18 = __cached_0;
-      vec_s8x16 __cached_1;
-      __cached_1 = vec_s8x16::load(&__ins_0[((((((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) % 16UL) * 4UL) + (((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 16UL) % 16UL) * 64UL)) + 1UL) * 1024UL) + (_fuseiter_306 + ((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 256UL) * 64UL)))]);
-      row2_19 = __cached_1;
-      vec_s8x16 __cached_2;
-      __cached_2 = vec_s8x16::load(&__ins_0[((((((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) % 16UL) * 4UL) + (((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 16UL) % 16UL) * 64UL)) + 2UL) * 1024UL) + (_fuseiter_306 + ((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 256UL) * 64UL)))]);
-      row3_20 = __cached_2;
-      vec_s8x16 __cached_3;
-      __cached_3 = vec_s8x16::load(&__ins_0[((((((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) % 16UL) * 4UL) + (((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 16UL) % 16UL) * 64UL)) + 3UL) * 1024UL) + (_fuseiter_306 + ((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 256UL) * 64UL)))]);
-      row4_21 = __cached_3;
-      vec_u8x16 xmm0;
-      vec_u8x16 xmm1;
-      vec_u8x16 xmm2;
-      vec_u8x16 xmm3;
-      vec_u8x16 xmm_tmp;
-      xmm0 = sc_reinterpret<vec_u8x16>(row1_18);
-      xmm1 = sc_reinterpret<vec_u8x16>(row2_19);
-      xmm2 = sc_reinterpret<vec_u8x16>(row3_20);
-      xmm3 = sc_reinterpret<vec_u8x16>(row4_21);
-      xmm_tmp = xmm0;
-      xmm0 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(285282304UL, 318968322UL, 352654340UL, 386340358UL)), xmm1);
-      xmm1 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(420026376UL, 453712394UL, 487398412UL, 521084430UL)), xmm1);
-      xmm_tmp = xmm2;
-      xmm2 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(285282304UL, 318968322UL, 352654340UL, 386340358UL)), xmm3);
-      xmm3 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(420026376UL, 453712394UL, 487398412UL, 521084430UL)), xmm3);
-      xmm_tmp = xmm0;
-      xmm0 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(286261504UL, 319947522UL, 353633540UL, 387319558UL)), xmm2);
-      xmm2 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(421005576UL, 454691594UL, 488377612UL, 522063630UL)), xmm2);
-      xmm_tmp = xmm1;
-      xmm1 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(286261504UL, 319947522UL, 353633540UL, 387319558UL)), xmm3);
-      xmm3 = sc_permutex2var(xmm_tmp, sc_reinterpret<vec_u8x16>(vec_u32x4(421005576UL, 454691594UL, 488377612UL, 522063630UL)), xmm3);
-      xmm_tmp = xmm1;
-      xmm1 = xmm2;
-      xmm2 = xmm_tmp;
-      row1_18 = sc_reinterpret<vec_s8x16>(xmm0);
-      row2_19 = sc_reinterpret<vec_s8x16>(xmm1);
-      row3_20 = sc_reinterpret<vec_s8x16>(xmm2);
-      row4_21 = sc_reinterpret<vec_s8x16>(xmm3);
-      vec_s8x16 __cached_4;
-      __cached_4 = row1_18;
-      vec_s8x16::store(__cached_4, &__outs_0[(((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 256UL) * 65536UL) + ((((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 16UL) % 16UL) * 4096UL) + (((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) % 16UL) * 256UL) + (_fuseiter_306 * 4UL))))]);
-      vec_s8x16 __cached_5;
-      __cached_5 = row2_19;
-      vec_s8x16::store(__cached_5, &__outs_0[(((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 256UL) * 65536UL) + ((((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 16UL) % 16UL) * 4096UL) + (((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) % 16UL) * 256UL) + ((_fuseiter_306 + 4UL) * 4UL))))]);
-      vec_s8x16 __cached_6;
-      __cached_6 = row3_20;
-      vec_s8x16::store(__cached_6, &__outs_0[(((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 256UL) * 65536UL) + ((((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 16UL) % 16UL) * 4096UL) + (((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) % 16UL) * 256UL) + ((_fuseiter_306 + 8UL) * 4UL))))]);
-      vec_s8x16 __cached_7;
-      __cached_7 = row4_21;
-      vec_s8x16::store(__cached_7, &__outs_0[(((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 256UL) * 65536UL) + ((((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) / 16UL) % 16UL) * 4096UL) + (((((fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0outer * 4UL) + fused_0fused_0_fuseiter_303___fuseiter_304_6___fuseiter_305_7_0inner) % 16UL) * 256UL) + ((_fuseiter_306 + 12UL) * 4UL))))]);
-    }
-  }
-}
-
-static void reorder__300_closure_11_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  reorder__300_closure_11(__stream, __module_data, i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
-}
-
-static void outerloop_4096_partition_cast_reduce_mul_reorder_390_closure_12(void* __stream, int8_t* __restrict__ __module_data, uint64_t __itr_0_0outer, uint8_t* __restrict__ cast_38_ins_0, int32_t* __restrict__ mul_41_ins_1, int32_t* __restrict__ reorder_49_outs_0) noexcept{
-  int8_t* __rescheduled_0 = (int8_t*)sc_thread_aligned_malloc(__stream, 4096UL);
-  int32_t* cast_38_outs_0_shr = (int32_t*)&__rescheduled_0[0UL];
-  for (uint64_t __itr_0_0inner = 0UL; __itr_0_0inner < 4UL; __itr_0_0inner += 1UL) {
-    vec_s32x16 reduce__10;
-    for (uint64_t _fuseiter59 = 0UL; _fuseiter59 < 1024UL; _fuseiter59 += 16UL) {
-      vec_u8x16 __cached_0;
-      __cached_0 = vec_u8x16::load(&cast_38_ins_0[((((__itr_0_0outer * 4UL) + __itr_0_0inner) * 1024UL) + _fuseiter59)]);
-      vec_s32x16 __cached_1;
-      __cached_1 = (vec_s32x16)(__cached_0);
-      vec_s32x16::store(__cached_1, &cast_38_outs_0_shr[_fuseiter59]);
-    }
-    reduce__10 = vec_s32x16(0);
-    for (uint64_t _fuseiter_62 = 0UL; _fuseiter_62 < 1024UL; _fuseiter_62 += 16UL) {
-      vec_s32x16 __cached_2;
-      __cached_2 = vec_s32x16::load(&cast_38_outs_0_shr[_fuseiter_62]);
-      reduce__10 = (__cached_2 + reduce__10);
-    }
-    int32_t __cached_3;
-    __cached_3 = sc_reduce_add(reduce__10);
-    int32_t __cached_4;
-    __cached_4 = mul_41_ins_1[0];
-    __cached_3 = (__cached_3 * __cached_4);
-    int32_t __cached_5;
-    __cached_5 = __cached_3;
-    reorder_49_outs_0[(((((__itr_0_0outer * 4UL) + __itr_0_0inner) / 32UL) * 32UL) + (((__itr_0_0outer * 4UL) + __itr_0_0inner) % 32UL))] = __cached_5;
-  }
-  sc_thread_aligned_free(__stream, __rescheduled_0);
-}
-
-static void outerloop_4096_partition_cast_reduce_mul_reorder_390_closure_12_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  outerloop_4096_partition_cast_reduce_mul_reorder_390_closure_12(__stream, __module_data, i, (uint8_t*)(args[0UL].v_ptr), (int32_t*)(args[1UL].v_ptr), (int32_t*)(args[2UL].v_ptr));
-}
-
-static void outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder_120_closure_13(void* __stream, int8_t* __restrict__ __module_data, uint64_t m_s_0, uint8_t* __restrict__ quantized_managed_matmul_core_2_ins_0, int8_t* __restrict__ quantized_managed_matmul_core_2_ins_1, int32_t* __restrict__ sub_4_ins_1, int32_t* __restrict__ sub_6_ins_1, float* __restrict__ mul_9_ins_1, float* __restrict__ add_12_ins_1, int32_t* __restrict__ mul_17_ins_1, int8_t* __restrict__ quantized_managed_matmul_core_19_ins_1, int32_t* __restrict__ sub_22_ins_1, float* __restrict__ mul_25_ins_1, float* __restrict__ add_28_ins_1, uint8_t* __restrict__ reorder_31_outs_0) noexcept{
-  void*& __sc_kernel_cache = *(void**)(__module_data + 8);
-  void*& __sc_kernel_cache_1 = *(void**)(__module_data + 16);
-  int8_t* __rescheduled_0 = (int8_t*)sc_thread_aligned_malloc(__stream, 43136UL);
-  uint8_t* cast_13_outs_0_shr = (uint8_t*)&__rescheduled_0[2048UL];
-  int32_t* mul_17_outs_0_shr = (int32_t*)&__rescheduled_0[34816UL];
-  int32_t* reduce_compute_32_outs_0_shr = (int32_t*)&__rescheduled_0[0UL];
-  for (uint64_t itr = 0UL; itr < 512UL; itr += 16UL) {
-    vec_s32x16 __cached_0;
-    __cached_0 = vec_s32x16(0);
-    vec_s32x16::store(__cached_0, &reduce_compute_32_outs_0_shr[itr]);
-  }
-  int32_t tid = sc_get_thread_id();
-  int32_t m_o_end = 1;
-  int32_t n_o_end = 16;
-  int32_t* quantized_managed_matmul_core_2_outs_0_shr = (int32_t*)&__rescheduled_0[34816UL];
-  for (uint64_t n_o = 0UL; n_o < 16UL; n_o += 1UL) {
-    uint64_t m_start_idx = (m_s_0 * 32UL);
-    uint64_t n_start_idx = (((n_o + (uint64_t)tid) % 16UL) * 64UL);
-    int32_t bs = 16;
-    uint64_t k_start_idx = 0UL;
-    dnnl_brgemm_call(__sc_kernel_cache, &quantized_managed_matmul_core_2_ins_0[(m_start_idx * 1024UL)], &quantized_managed_matmul_core_2_ins_1[((n_start_idx / 64UL) * 65536UL)], &quantized_managed_matmul_core_2_outs_0_shr[0UL], bs, __stream);
-    for (uint64_t __inner_itr_2 = 0UL; __inner_itr_2 < 32UL; __inner_itr_2 += 1UL) {
-      for (uint64_t _fuseiter_159 = 0UL; _fuseiter_159 < 64UL; _fuseiter_159 += 16UL) {
-        vec_s32x16 __cached_1;
-        __cached_1 = vec_s32x16::load(&quantized_managed_matmul_core_2_outs_0_shr[((__inner_itr_2 * 64UL) + _fuseiter_159)]);
-        vec_s32x16 __cached_2;
-        __cached_2 = (__cached_1 - vec_s32x16(sub_4_ins_1[(((m_start_idx / 32UL) * 32UL) + __inner_itr_2)]));
-        vec_s32x16 __cached_3;
-        __cached_3 = vec_s32x16::load(&sub_6_ins_1[(((n_start_idx / 64UL) * 64UL) + _fuseiter_159)]);
-        __cached_2 = (__cached_2 - __cached_3);
-        vec_f32x16 __cached_4;
-        __cached_4 = (vec_f32x16)(__cached_2);
-        __cached_4 = (__cached_4 * vec_f32x16(mul_9_ins_1[0]));
-        __cached_4 = sc_max(__cached_4, vec_f32x16(0.f));
-        __cached_4 = (__cached_4 + vec_f32x16(add_12_ins_1[0]));
-        vec_u8x16 __cached_5;
-        __cached_5 = sc_saturated_cast<vec_u8x16>(sc_round_and_cast<vec_s32x16>(sc_max(__cached_4, vec_f32x16(0.f))));
-        vec_u8x16::store(__cached_5, &cast_13_outs_0_shr[(((((m_start_idx / 32UL) - m_s_0) * 32768UL) + (((n_start_idx / 64UL) * 2048UL) + (__inner_itr_2 * 64UL))) + _fuseiter_159)]);
-        vec_s32x16 __cached_6;
-        __cached_6 = (vec_s32x16)(__cached_5);
-        vec_s32x16 __cached_7;
-        __cached_7 = vec_s32x16::load(&reduce_compute_32_outs_0_shr[((((m_start_idx / 32UL) - m_s_0) * 512UL) + (__inner_itr_2 * 16UL))]);
-        __cached_7 = (__cached_7 + __cached_6);
-        vec_s32x16::store(__cached_7, &reduce_compute_32_outs_0_shr[((((m_start_idx / 32UL) - m_s_0) * 512UL) + (__inner_itr_2 * 16UL))]);
-      }
-    }
-  }
-  int32_t tid_1 = sc_get_thread_id();
-  int32_t m_o_end_2 = 1;
-  int32_t n_o_end_3 = 16;
-  for (uint64_t __inner_itr_2_4 = 0UL; __inner_itr_2_4 < 32UL; __inner_itr_2_4 += 1UL) {
-    int32_t __cached_8;
-    __cached_8 = 0;
-    vec_s32x16 __cached_9;
-    __cached_9 = vec_s32x16::load(&reduce_compute_32_outs_0_shr[(__inner_itr_2_4 * 16UL)]);
-    int32_t __cached_10;
-    __cached_10 = sc_reduce_add(__cached_9);
-    int32_t __cached_11;
-    __cached_11 = mul_17_ins_1[0];
-    mul_17_outs_0_shr[__inner_itr_2_4] = (__cached_10 * __cached_11);
-  }
-  int32_t* quantized_managed_matmul_core_19_outs_0_shr = (int32_t*)&__rescheduled_0[34944UL];
-  for (uint64_t n_o_5 = 0UL; n_o_5 < 16UL; n_o_5 += 1UL) {
-    uint64_t m_start_idx_6 = (m_s_0 * 32UL);
-    uint64_t n_start_idx_7 = (((n_o_5 + (uint64_t)tid_1) % 16UL) * 64UL);
-    int32_t bs_8 = 16;
-    uint64_t k_start_idx_9 = 0UL;
-    dnnl_brgemm_call(__sc_kernel_cache_1, &cast_13_outs_0_shr[(((m_start_idx_6 / 32UL) - m_s_0) * 32768UL)], &quantized_managed_matmul_core_19_ins_1[((n_start_idx_7 / 64UL) * 65536UL)], &quantized_managed_matmul_core_19_outs_0_shr[0UL], bs_8, __stream);
-    for (uint64_t __inner_itr_2_10 = 0UL; __inner_itr_2_10 < 32UL; __inner_itr_2_10 += 1UL) {
-      for (uint64_t _fuseiter_214 = 0UL; _fuseiter_214 < 64UL; _fuseiter_214 += 16UL) {
-        vec_s32x16 __cached_12;
-        __cached_12 = vec_s32x16::load(&quantized_managed_matmul_core_19_outs_0_shr[((__inner_itr_2_10 * 64UL) + _fuseiter_214)]);
-        vec_s32x16 __cached_13;
-        __cached_13 = (__cached_12 - vec_s32x16(mul_17_outs_0_shr[((((m_start_idx_6 / 32UL) - m_s_0) * 32UL) + __inner_itr_2_10)]));
-        vec_s32x16 __cached_14;
-        __cached_14 = vec_s32x16::load(&sub_22_ins_1[(((n_start_idx_7 / 64UL) * 64UL) + _fuseiter_214)]);
-        __cached_13 = (__cached_13 - __cached_14);
-        vec_f32x16 __cached_15;
-        __cached_15 = (vec_f32x16)(__cached_13);
-        __cached_15 = (__cached_15 * vec_f32x16(mul_25_ins_1[0]));
-        __cached_15 = sc_max(__cached_15, vec_f32x16(0.f));
-        __cached_15 = (__cached_15 + vec_f32x16(add_28_ins_1[0]));
-        vec_u8x16 __cached_16;
-        __cached_16 = sc_saturated_cast<vec_u8x16>(sc_round_and_cast<vec_s32x16>(sc_max(__cached_15, vec_f32x16(0.f))));
-        vec_u8x16 __cached_17;
-        __cached_17 = __cached_16;
-        vec_u8x16::store(__cached_17, &reorder_31_outs_0[(((__inner_itr_2_10 + ((m_start_idx_6 / 32UL) * 32UL)) * 1024UL) + (_fuseiter_214 + ((n_start_idx_7 / 64UL) * 64UL)))]);
-      }
-    }
-  }
-  sc_thread_aligned_free(__stream, __rescheduled_0);
-}
-
-static void outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder_120_closure_13_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
-  outerloop_128_partition_quantized_managed_matmul_core_sub_sub_cast_mul_relu_add_cast_quantized_managed_matmul_core_cast_reduce_compute_reduce_collect_mul_sub_sub_cast_mul_relu_add_cast_reorder_120_closure_13(__stream, __module_data, i, (uint8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr), (int32_t*)(args[2UL].v_ptr), (int32_t*)(args[3UL].v_ptr), (float*)(args[4UL].v_ptr), (float*)(args[5UL].v_ptr), (int32_t*)(args[6UL].v_ptr), (int8_t*)(args[7UL].v_ptr), (int32_t*)(args[8UL].v_ptr), (float*)(args[9UL].v_ptr), (float*)(args[10UL].v_ptr), (uint8_t*)(args[11UL].v_ptr));
-}
-
-extern "C" void mlp_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept{
-  sc::runtime::thread_manager::cur_mgr.run_main_function((sc::runtime::thread_manager::main_func_t)mlp_0wrapper_impl, (sc::runtime::stream_t *)__stream, __module_data, args);
-}
\ No newline at end of file
diff --git a/src/mlp.hpp b/src/mlp.hpp
deleted file mode 100644
index 14ca6ed..0000000
--- a/src/mlp.hpp
+++ /dev/null
@@ -1,8 +0,0 @@
-#include <stdint.h>
-#include <runtime/generic_val.hpp>
-using generic_val = sc::generic_val;
-
-extern uint8_t mlp_data[2105536];
-
-extern "C" void sc_init_mlp(void* __stream, int8_t* __restrict__ __module_data) noexcept __attribute__((nonnull (2)));
-extern "C" void mlp_0wrapper(void* __stream, int8_t* __restrict__ __module_data, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,3)));
diff --git a/src/mlp_data.cpp b/src/mlp_data.cpp
deleted file mode 100644
index 9bd1eef..0000000
--- a/src/mlp_data.cpp
+++ /dev/null
@@ -1,3 +0,0 @@
-#include <stdint.h>
-
-alignas(64) uint8_t mlp_data[2105536] = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,0,251,255,255,255,128,255,255,255,0,0,10,0,251,255,255,255,128,255,255,255,0,0,160,192,0,0,160,64,11,215,163,189,0,0,160,192,0,0,160,64,11,215,163,189,128,0,0,0,128,0,0,0,};
